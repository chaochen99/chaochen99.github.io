<!DOCTYPE htML>
<htML lang="zxx">

<head >

	<title>从国际会议参与情况分析中国人工智能的发展水平</title>
	<!-- Meta tag Keywords -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta charset="UTF-8" />
	<meta name="keywords" content="" />
	<script>
		addEventListener("load", function() {
			setTimeout(hideURLbar, 0);
		}, false);

		function hideURLbar() {
			window.scrollTo(0, 1);
		}
	</script>
	<!-- //Meta tag Keywords -->
	<!-- Custom-Files -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Bootstrap-Core-CSS -->
	<link rel="stylesheet" href="css/style.css" type="text/css" media="all" />
	<!-- Style-CSS -->
	<!-- font-awesome-icons -->
	<link href="css/font-awesome.css" rel="stylesheet">
	<!-- //font-awesome-icons -->
	<!-- /Fonts -->
	<link href="http://fonts.googleapis.com/css?family=Poppins:100,100i,200,200i,300,300i,400,400i,500,500i,600,600i,700,700i,800" rel="stylesheet">
	<link rel="stylesheet" href="style.css">
	<!-- //Fonts -->

	<script src="js/echarts.min.js"></script>

	<style>
		p{
			text-align: center;
		}
        table
        {
            border-collapse: collapse;
            margin: 0 auto;
			text-align: center;
			vertical-align:center
        }
        table td, table th
        {
            border: 1px solid #cad9ea;
            color: #666;
			height: 30px;
			vertical-align:top
			

		}
        table thead th
        {
            background-color: #CCE8EB;
			width: 100px;
			vertical-align:top
			
        }
        table tr:nth-child(odd)
        {
            background: #fff;
        }
        table tr:nth-child(even)
        {
            background: #F5FAFA;
        }
	</style>
</head>

<body>

<div style="position: fixed;background:white;top: 0px;width: 100%;z-index: 9999">
	<div class="container d-lg-flex" >
		<!-- logo -->
		<!-- //logo -->
		<!-- nav -->
		<div class="nav_w3pvt mx-lg-auto">
			<nav>
				<label for="drop" class="toggle">Menu</label>
				<input type="checkbox" id="drop" />
				<ul class="menu mx-lg-auto">
					<li><a href="index.htML" class="active">主页</a></li>
					<li>
						<!-- First Tier Drop Down -->
						<label for="drop-2" class="toggle toogle-2">Pages <span class="fa fa-angle-down" aria-hidden="true"></span>
						</label>
						<a href="#">Pages <span class="fa fa-angle-down" aria-hidden="true"></span></a>
						<input type="checkbox" id="drop-2" />
						<ul>
							<li><a href="#lunwenshuliang" class="drop-text">论文数量分析</a></li>
							<li><a href="#lunwenyingxiangli" class="drop-text">论文影响力分析</a></li>
							<li><a href="#yanjiudanwei_daohang" class="drop-text">研究单位分布情况分析</a></li>
							<li><a href="#guoji" class="drop-text">国际学术合作情况分析</a></li>
							<li><a href="#xiaoqilunwen" class="drop-text">校企合作情况分析</a></li>
							<li><a href="#about-guanjianci" class="drop-text">研究热点分析</a></li>
							<li><a href="#fanzhanbuju" class="drop-text">研究方向发展布局分析</a></li>

							
							
						</ul>
					</li>
					<li><a href="#jielun" class="drop-text">结论</a></li>
					<li><a href="#cankaowenxian" class="drop-text">参考文献</a></li>
					<li><a href="#test1">Thanks</a></li>

					<li><a href="#contact">联系我们</a></li>
				</ul>
			</nav>
		</div>

		<!-- //dwn -->
	</div>
</div>
<!-- //navigation -->
<!-- //header 2 -->

<!-- banner -->
<div id="home" class="banner_main" >
	<div class="overlay-w3pvt">
		<div class="container">
			<div class="banner-info text-center">
				<h3>从国际会议参与情况分析中国人工智能的发展水平</h3>
				<h5 style="padding-top: 90px;font-size: 30px">Chaochen Gao</h5>

			</div>
		</div>
	</div>
</div>
<!-- //banner -->
<section class=" py-5" id="zhaiyao"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h3>摘要</h3>
				<p > 本文分析了在1990年到2019年期间人工智能方向的CCF推荐国际会议的论文数据。以中国学者引领的研究成果为出发点，采用数据挖掘、分析学习、数据可视化等方法，从论文数量和影响力、科研人员分布及合作情况、研究热点三个角度分析了论文发表情况。此外，阐述了AI各研究方向发展布局情况，分析出优势、发展中、潜力和弱势研究方向。数据分析结果表明，中国人工智能研究在2005年之后进入快速增长时段，包括计算机视觉、自然语言处理方向的论文数量开始大幅增长，论文质量和影响力也随之大幅提高，并逐渐发展为我国的优势研究方向。此外，中国在加强国际合作、增加国际合作论文产出的同时，独立研究能力有显著提高。</p>

			</div>

		</div>

	</div>
</section>
<div id="shujulaiyuanjichuli" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 数据来源及处理</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>


<section class=" py-5" id="shujulaiyuan"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">

				<p 	>本文的分析对象为由清华大学提供的CCF推荐国际会议论文数据，共620312行。包含论文id，英文标题，中文标题，发表年份，引用数，关键词，发表会议，摘要，pdf链接，作者id，姓名，中文姓名，国家，单位，单位类型等字段，其中部分值丢失。
					在原始数据中，有很多论文发表在非主会会议，为保证论文质量，对论文按照发表会议进行筛选，删除掉的论文数据有三种：</br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp1.	属于demo，poster， short paper， student session，Symposium，tutorial， workshop等性质的论文数据。比如“CVPR Workshops”，“Proceedings of the ACL 2011 Student Session”等。</br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp2.	发表会议非CCF推荐会议，即论文所属会议不在《中国计算机学会推荐国际学术会议和期刊目录-2019》（详见参考文献2）的人工智能推荐会议板块。</br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp3.	会议名称丢失。</br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp经过去除噪音之后将剩余论文会议名称简化，
					比如将“Proceedings of the 27th AAAI Conference on Artificial Intelligence,  AAAI 2013”，“AAAI”等映射为AAAI。
					最终共得到有效论文数据92910篇。历史发文数量前六名的会议发文数量历史如下：<br>
				</p>	
				<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-huiyi" >

				</div>
				<script>
					var myChart_huiyi = echarts.init(document.getElementById('echarts-huiyi'));
	
					setTimeout(function () {
	
						option = {
							legend: {type: 'scroll',
											top:45,},
							title:[
								{
								text:'各大会议论文发表数量及占比',
								left:'center',
								
							},
							],
							tooltip: {
								trigger: 'axis',
								axisPointer: {
									type: 'shadow',
									label: {
										show: true
									}
								}
							},
							toolbox: {
								show: true,
								feature: {
									mark: {show: true},
									dataView: {show: true, readOnly: false},
									magicType: {show: true, type: ['line', 'bar']},
									restore: {show: true},
									saveAsImage: {show: true}
								}
							},
							calculable: true,
							dataZoom: [
								{
									show: true,
									start: 0,
									end: 100
								},
								{
									type: 'inside',
									start: 94,
									end: 100
								},
							],
							dataset: {
								source: [
								['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
											'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
											'2013','2014','2015','2016','2017','2018','2019'],
									['ICRA', 327.0,379.0,421.0,434.0,518.0,511.0,570.0,567.0,596.0,515.0,822.0,865.0,1063.0,907.0,1059.0,759.0,670.0,789.0,654.0,723.0,834.0,987.0,819.0,854.0,1020.0,925.0,694.0,798.0,825.0,1022.0],
									['AAAI', 194.0,153.0,145.0,153.0,311.0,9.0,301.0,204.0,221.0,189.0,236.0,6.0,185.0,12.0,194.0,337.0,405.0,381.0,364.0,16.0,319.0,319.0,354.0,393.0,694.0,889.0,851.0,1007.0,1220.0,1306.0],
									['IJCAI', 6.0,195.0,12.0,247.0,4.0,288.0,4.0,241.0,15.0,219.0,1.0,198.0,7.0,291.0,9.0,355.0,7.0,513.0,7.0,391.0,12.0,496.0,16.0,507.0,16.0,686.0,660.0,785.0,888.0,953.0],
									['CVPR',13.0,137.0,155.0,177.0,157.0,12.0,138.0,172.0,140.0,196.0,221.0,12.0,2.0,3.0,2.0,1.0,2.0,525.0,497.0,343.0,453.0,427.0,461.0,464.0,540.0,614.0,646.0,798.0,981.0,1253.0],
									['NIPS', 277.0,133.0,149.0,145.0,132.0,163.0,155.0,51.0,164.0,258.0,157.0,222.0,224.0,212.0,234.0,236.0,219.0,246.0,263.0,318.0,324.0,319.0,385.0,365.0,426.0,435.0,574.0,687.0,3.0,1.0],
									['ICCV', 115.0,6.0,9.0,92.0,1.0,160.0,4.0,3.0,176.0,175.0,12.0,266.0,5.0,190.0,8.0,300.0,3.0,384.0,2.0,290.0,5.0,364.0,1.0,457.0,10.0,528.0,8.0,629.0,1.0,1075.0]
								]
							},
							xAxis: {type: 'category',name:'年份（年）'},
							yAxis: {gridIndex: 0,name:'数量（篇）'},
							grid: {width: '50%',left:'5%',top:100,},
							series: [
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{
									
									type: 'pie',
									id: 'pie',
									center:['78%','55%'],
									radius: ['20%', '35%'],
									avoidLabelOverlap: false,
									label: {
											show: true,
											position:'outside',
											formatter: '{b}:{@1990}({d}%)',

										},
									encode: {
										itemName: 'product',
										value: '1990',
										tooltip: '1990'
									},
									emphasis: {
										label: {
											show: true,
											fontWeight: 'bold',
											fontSize:15
										}
									},

								}
							]
						};
	
						myChart_huiyi.on('updateAxisPointer', function (event) {
							var xAxisInfo = event.axesInfo[0];
							if (xAxisInfo) {
								var dimension = xAxisInfo.value + 1;
								myChart_huiyi.setOption({
									series: {
										id: 'pie',
										label: {
											formatter: '{b}:{@1990}({d}%)'
										},
										encode: {
											value: dimension,
											tooltip: dimension
										}
									}
								});
							}
						});
	
						myChart_huiyi.setOption(option);
	
					});
	
				</script>
				<P>可以发现，在2015年及之前，机器人领域论文的ICRA会议占比一直处于第一位，说明当时机器人领域处于热点地位，IJCAI和ICCV会议是隔年举办的会议，呈周期性上涨趋势。CVPR会议在2006年之后呈增长趋势。NIPS在1997年之后开始呈缓慢上升态势。
					AAAI会议从2003年之后整体呈上升趋势，在2013年之后增长趋势变快，并2016年超过ICRA会议，成为发表论文数量最多的会议。2019年时，除NIPS会议外其他会议发文数量相差不大，呈现“百家争鸣”局面。</P>

				<p>

					本次实验将所有论文分为六类方向， 分别为:1.机器学习和神经网络(ML)，2.计算机视觉(CV)，3.自然语言处理(NLP)，4.机器人(ROBOT)，5.智能体(AGENT)，6.知识表示(KR)。 
					由于一些会议已经具备鲜明的方向，则可以直接将这些会议的论文划分到对应的方向，
					使用已有方向的论文训练模型之后对综合类会议的论文进行分类。 各方向的会议如下表所示:
				</p>
				<table style="width: 60%;" class="table">
					<thead>
						<tr>
							<th>
								会议类别
							</th>
							<th>
								会议简写
							</th>
						</tr>
					</thead>
					<tr>
						<td>
							综合类
						</td>
						<td>
							AAAI、IJCAI、ECAI、ICAPS
						</td>
					</tr>
					<tr>
						<td>
							机器学习和神经网络
						</td>
						<td>
							ICML、NIPS、COLT、NeurIPS
						</td>
			
					</tr>
					<tr>
						<td>
							计算机视觉
						</td>
						<td>
							ICCV、CVPR、ECCV
						</td>
			
					</tr>
					<tr>
						<td>
							自然语言处理
						</td>
						<td>
							ACL、EMNLP、COLING
						</td>
					</tr>
					<tr>
						<td>
							机器人
						</td>
						<td>
							ICRA
						</td>
			
					</tr>
					<tr>
						<td>
							智能体
						</td>
						<td>
							AAMAS
						</td>
			
					</tr>
					<tr>
						<td>
							知识表示
						</td>
						<td>
							KR
						</td>
			
					</tr>
				</table>
				<p>
					
					其中，非综合类会议共有44062篇论文，将非综合类数据根据4:1划分得到30843条数据的训练集和8813条数据的测试集。训练逻辑回归模型之后对综合类会议论文进行分类，最后得到机器学习和神经网络方向论文21712篇，
					计算机视觉方向论文17594篇，自然语言处理方向论文16638篇，机器人方向论文27031篇，知识表示方向论文2681篇，智能体方向论文7254篇。各个方向历史发文情况如下：
				</p>
								<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-zhutibianhua" >

				</div>
				<script>
					var myChart = echarts.init(document.getElementById('echarts-zhutibianhua'));

					setTimeout(function () {

						option = {
							legend: {type: 'scroll',
										top:45,},
							title:{
								text:'各方向发表论文总量及占比',
								left:'center',
								top:'1%',
							},
							tooltip: {
								trigger: 'axis',
								axisPointer: {
									type: 'shadow',
									label: {
										show: true
									}
								}
							},
							toolbox: {
								show: true,
								feature: {
									mark: {show: true},
									dataView: {show: true, readOnly: false},
									magicType: {show: true, type: ['line', 'bar']},
									restore: {show: true},
									saveAsImage: {show: true}
								}
							},
							calculable: true,
							dataZoom: [
								{
									show: true,
									start: 0,
									end: 100
								},
								{
									type: 'inside',
									start: 94,
									end: 100
								},

							],
							dataset: {
								source: [
									['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
									['ML', 333.0,208.0,214.0,354.0,292.0,351.0,363.0,286.0,326.0,465.0,411.0,402.0,406.0,451.0,444.0,635.0,569.0,717.0,590.0,682.0,744.0,781.0,991.0,1069.0,1120.0,1640.0,1328.0,1981.0,1586.0,1133.0],
									['CV', 131.0,155.0,170.0,285.0,170.0,214.0,151.0,188.0,322.0,382.0,238.0,286.0,15.0,209.0,24.0,342.0,21.0,978.0,514.0,679.0,468.0,838.0,487.0,1016.0,599.0,1302.0,890.0,1678.0,1412.0,2803.0],
									['NLP', 294.0,152.0,301.0,147.0,341.0,116.0,344.0,180.0,333.0,209.0,333.0,144.0,349.0,192.0,398.0,352.0,317.0,410.0,466.0,349.0,512.0,456.0,539.0,704.0,886.0,954.0,1201.0,1002.0,1784.0,408.0],
									['ROBOT',373.0,470.0,452.0,506.0,592.0,563.0,644.0,675.0,643.0,592.0,871.0,885.0,1110.0,1005.0,1093.0,866.0,732.0,901.0,731.0,772.0,890.0,1113.0,894.0,950.0,1225.0,1134.0,861.0,1026.0,1030.0,1211.0],
									['AGENT', 20.0,32.0,10.0,42.0,41.0,35.0,48.0,53.0,53.0,70.0,49.0,59.0,287.0,272.0,316.0,389.0,334.0,471.0,82.0,107.0,342.0,452.0,341.0,466.0,437.0,614.0,573.0,479.0,536.0,125.0],
									['KR',37.0,117.0,85.0,57.0,95.0,49.0,87.0,46.0,74.0,49.0,86.0,27.0,65.0,33.0,95.0,67.0,76.0,79.0,104.0,37.0,86.0,84.0,87.0,82.0,103.0,120.0,151.0,71.0,151.0,52.0]
								]
							},
							xAxis: {type: 'category',name:'年份（年）'},
							yAxis: {gridIndex: 0,name:'数量（篇）'},
							grid: {width: '50%',left:'5%',top:100,},
							series: [
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{
									
									type: 'pie',
									id: 'pie',
									center:['78%','55%'],
									radius: ['20%', '35%'],
									avoidLabelOverlap: false,
									label: {
											show: true,
											position:'outside',
											formatter: '{b}:{@1990}({d}%)',

										},
									encode: {
										itemName: 'product',
										value: '1990',
										tooltip: '1990'
									},
									emphasis: {
										label: {
											show: true,
											fontWeight: 'bold',
											fontSize:15
										}
									},

								}
							]
						};

						myChart.on('updateAxisPointer', function (event) {
							var xAxisInfo = event.axesInfo[0];
							if (xAxisInfo) {
								var dimension = xAxisInfo.value + 1;
								myChart.setOption({
									series: {
										id: 'pie',
										label: {
											formatter: '{b}:{@1990}({d}%)'
										},
										encode: {
											value: dimension,
											tooltip: dimension
										}
									}
								});
							}
						});

						myChart.setOption(option);

					});

				</script>
				<p>自然语言处理方向从2011年之后，发表论文数量有大幅增长，计算机视觉在2006年呈现转折点，此后开始呈现整体增长趋势，根据之前对会议统计结果，
					可以发现这种现象与计算机视觉的国际会议有关,此类会议的隔年举办导致周期性波动，但AAAI，IJCAI等综合类会议中的计算机视觉方向论文数量抵消了部分影响，
					在2019年，计算机视觉成为论文发表最多的方向。
					机器人方向于1990年就处于领先地位，2007年被计算机视觉首次超越。
					智能体和知识表示一直处于低占比低增长的态势。
					</p>
			</div>

		</div>

	</div>
</section>


<!-- //banner bottom grids -->
<!-- banner bottom grids -->

<div id="lunwenshuliang" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 论文数量分析</h4>
<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>



<section class=" py-5" id="jieshi1"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p>本文中国发表的论文指第一作者所属国家及第一作者所在第一单位所属国家皆为中国的论文。</p>
				<p>论文发表数量是测度科研水平的重要指标，本节统计了1990-2019年间中国在各个方向论文发表数量占比。</p>
				<p >结果显示，在2005年之前，中国在各个方向的论文发表占比相差不大，占比较低且比较平稳，在2005年之后，计算机视觉和自然语言处理开始分离出来，不断走高。
					机器学习和神经网络方向在2005年有较大幅度增长后呈现平稳趋势，并于2015年之后再次大幅度增长，
					智能体，机器人和知识表示一直处于低占比的平稳趋势。
				</p>
				<p>2005年之后中国主导的机器学习和神经网络，计算机视觉，自然语言处理方向的论文数量开始增长，尤其是计算机视觉和自然语言处理，在2019年占比已经分别达到了43%和34%,
				和世界范围内计算机视觉，自然语言处理发展趋势相吻合，说明中国的积极参与有力的推动了计算机视觉,自然语言处理领域的发展。</p>
			</div>

		</div>

	</div>
</section>
<section class=" py-5" id="about-fangxiang_china"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 中国在各方向发表论文占比</h4>

			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-fangxiang-china" >

			</div>
			<script>
				var myChart_fangxiang_china = echarts.init(document.getElementById('echarts-fangxiang-china'));

				setTimeout(function () {

					option = {

						legend: {type: 'scroll',
										top:25,},

						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							},
							{
								show: true,
								yAxisIndex: 0,
								filterMode: 'empty',
								width: 30,
								height: '80%',
								showDataShadow: false,
								left: '93%'
							}
						],
						dataset: {
							source: [
							['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
								['ML', 0.006006006006006006,0.014423076923076924,0.014018691588785047,0.014124293785310734,0.0136986301369863,0.019943019943019943,0.024793388429752067,0.013986013986013986,0.012269938650306749,0.034408602150537634,0.0340632603406326,0.04477611940298507,0.022167487684729065,0.03991130820399113,0.04054054054054054,0.02204724409448819,0.0421792618629174,0.08368200836820083,0.07796610169491526,0.10557184750733138,0.09811827956989247,0.0973111395646607,0.08879919273461151,0.09728718428437792,0.09196428571428572,0.08231707317073171,0.12575301204819278,0.127208480565371,0.15889029003783103,0.24536628420123566
								],
								['CV', 0.04580152671755725,0.025806451612903226,0.0058823529411764705,0.0456140350877193,0.023529411764705882,0.018691588785046728,0.06622516556291391,0.05319148936170213,0.052795031055900624,0.0549738219895288,0.06302521008403361,0.07692307692307693,0.0,0.06698564593301436,0.08333333333333333,0.06432748538011696,0.09523809523809523,0.12883435582822086,0.1517509727626459,0.0898379970544919,0.09401709401709402,0.13365155131264916,0.12525667351129363,0.19291338582677164,0.18864774624373956,0.23886328725038403,0.27191011235955054,0.22169249106078665,0.28328611898017,0.3353549768105601
								],
								['NLP',0.02040816326530612,0.006578947368421052,0.03986710963455149,0.013605442176870748,0.04105571847507331,0.02586206896551724,0.020348837209302327,0.027777777777777776,0.02702702702702703,0.023923444976076555,0.06006006006006006,0.006944444444444444,0.06017191977077364,0.03125,0.04271356783919598,0.02556818181818182,0.05993690851735016,0.07073170731707316,0.11158798283261803,0.1318051575931232,0.1171875,0.1206140350877193,0.1614100185528757,0.21022727272727273,0.17832957110609482,0.23165618448637318,0.2206494587843464,0.19461077844311378,0.24271300448430494,0.43137254901960786
								],
								['ROBOT',0.024128686327077747,0.02127659574468085,0.03982300884955752,0.045454545454545456,0.033783783783783786,0.056838365896980464,0.05745341614906832,0.05037037037037037,0.060653188180404355,0.08952702702702703,0.0574052812858783,0.08135593220338982,0.06666666666666667,0.11940298507462686,0.0878316559926807,0.08314087759815242,0.05191256830601093,0.08324084350721421,0.07250341997264022,0.07383419689119171,0.0348314606741573,0.09703504043126684,0.04697986577181208,0.04842105263157895,0.09306122448979592,0.04585537918871252,0.08013937282229965,0.07602339181286549,0.07475728155339806,0.08175061932287365
								],
								['AGENT', 0.0,0.0,0.0,0.0,0.024390243902439025,0.0,0.0,0.0,0.03773584905660377,0.014285714285714285,0.02040816326530612,0.01694915254237288,0.017421602787456445,0.025735294117647058,0.006329113924050633,0.007712082262210797,0.005988023952095809,0.01910828025477707,0.0,0.09345794392523364,0.02046783625730994,0.017699115044247787,0.017595307917888565,0.030042918454935622,0.041189931350114416,0.048859934853420196,0.07155322862129145,0.05010438413361169,0.041044776119402986,0.128],
								['KR',0.0,0.03418803418803419,0.03529411764705882,0.0,0.05263157894736842,0.04081632653061224,0.05747126436781609,0.0,0.0945945945945946,0.10204081632653061,0.023255813953488372,0.037037037037037035,0.06153846153846154,0.12121212121212122,0.09473684210526316,0.05970149253731343,0.11842105263157894,0.10126582278481013,0.11538461538461539,0.0,0.08139534883720931,0.07142857142857142,0.10344827586206896,0.036585365853658534,0.038834951456310676,0.1,0.0728476821192053,0.0,0.10596026490066225,0.11538461538461539
								]
							]
						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'占比'},
						// grid: {top: '55%'},
						series: [
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
						]
					};


					myChart_fangxiang_china.setOption(option);

				});

			</script>
		</div>

	</div>
</section>




<div id="lunwenyingxiangli" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 论文影响力分析</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class=" py-5" id="jieshi2"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p>论文的被引用情况是测度科研影响力的重要指标，本节以中国论文被引用数以及中国在影响力排名前1%，5%，10%，20%的论文占比表征中国论文质量情况，同时统计中国各研究方向在影响力排名前5%的论文中的占比情况。</p>
				<p >2005年及之后中国论文被引用数占比有整体提高的趋势，中国论文被引用数在2016年到达峰值，占比达34.4%。
					在高影响力论文占比中，即便在前1%也有较大的比例，说明中国发表的论文整体质量较高，且包含影响力非常高的论文。
					</p>

			</div>
		</div>
	</div>
</section>
<section class=" py-5" id="about-effect"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 中国在影响力论文排名前1%,5%,10%,20%占比及中国论文被引用数占比</h4>
			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-effect" >

			</div>
			<script>
				var myChart_effect = echarts.init(document.getElementById('echarts-effect'));

				setTimeout(function () {

					option = {
						legend: {type: 'scroll',
										top:25,},
						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							},
							{
								show: true,
								yAxisIndex: 0,
								filterMode: 'empty',
								width: 30,
								height: '80%',
								showDataShadow: false,
								left: '93%'
							}
						],
						dataset: {
							source: [
								['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
								['1%',0.0,0.0,0.0,0.07692307692307693,0.0,0.0,0.0,0.0,0.0,0.0589,0.0,0.0,0.0,0.0,0.043478260869565216,0.038461538461538464,0.1,0.2,0.125,0.07692307692307693,0.1,0.1891891891891892,0.1875,0.21428571428571427,0.20930232558139536,0.21052631578947367,0.2,0.08064516129032258,0.15625,0.30434782608695654
								],
								['5%', 0.017241379310344827,0.0,0.01639344262295082,0.014492753623188406,0.02631578947368421,0.0,0.037037037037037035,0.028169014084507043,0.04597701149425287,0.045454545454545456,0.06060606060606061,0.03333333333333333,0.02702702702702703,0.04672897196261682,0.05084745762711865,0.030534351145038167,0.0594059405940594,0.1638418079096045,0.12096774193548387,0.07692307692307693,0.11920529801324503,0.0972972972972973,0.12195121951219512,0.1784037558685446,0.24311926605504589,0.2743055555555556,0.244,0.1607717041800643,0.14506172839506173,0.27586206896551724
								],
								['10%',0.008547008547008548,0.008928571428571428,0.01639344262295082,0.02158273381294964,0.02631578947368421,0.015151515151515152,0.03680981595092025,0.02112676056338028,0.04597701149425287,0.045454545454545456,0.050505050505050504,0.05,0.026905829596412557,0.06046511627906977,0.029661016949152543,0.034220532319391636,0.059113300492610835,0.1323943661971831,0.13306451612903225,0.07279693486590039,0.09900990099009901,0.13513513513513514,0.11854103343465046,0.17330210772833723,0.21330275229357798,0.24479166666666666,0.24,0.1781701444622793,0.16486902927580893,0.25862068965517243
								],
								['20%',0.01276595744680851,0.008888888888888889,0.02857142857142857,0.017985611510791366,0.02631578947368421,0.01893939393939394,0.03680981595092025,0.014035087719298246,0.054441260744985676,0.045454545454545456,0.05303030303030303,0.06388888888888888,0.03139013452914798,0.058004640371229696,0.046511627906976744,0.036053130929791274,0.051597051597051594,0.11971830985915492,0.12298387096774194,0.07265774378585087,0.08731466227347612,0.11081081081081082,0.10182370820668693,0.17076023391812867,0.15578465063001146,0.203125,0.239,0.1756214915797915,0.19399538106235567,0.26881720430107525
								],
								['被引用数',0.010433407641661051,0.007941653160453808,0.01894105225206665,0.11328512533345092,0.02002069917203312,0.012939463894484554,0.026329388443194428,0.015443370131118224,0.033193743466745335,0.05548205432877848,0.038091725718500564,0.03376434665645587,0.026000092547604173,0.04493093248011942,0.04112658200179239,0.03343238473864627,0.05028180235474638,0.1383444315865764,0.11689945415967044,0.0757847608826883,0.0870695096453509,0.12415027680078229,0.10552909004172518,0.14591366500517258,0.16732939743082462,0.2152675764035181,0.34425082832249354,0.160921320172899,0.1741799321323144,0.2617866004962779],
							]	

						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'占比'},
						// grid: {top: '55%'},
						series: [
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'}
						]
					};


					myChart_effect.setOption(option);

				});

			</script>
		</div>

	</div>
</section>
<section class=" py-5" id="jieshi3"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">

					<P>
					在影响力前5%的论文中，自然语言处理，计算机视觉2005年之前占比大部分时间在百分之十以下 ，近十年来突破百分之二十，到百分之三十。
					机器学习和神经网络2005年之前前的引用率占比在5%以下，2005后在5%～25%之间。
					机器人,智能体一直处于低占比的平稳态势，大部分时间占比在百分之十以下。

					</p>
			</div>
		</div>
	</div>
</section>
<section class=" py-5" id="about-fangxiang-effect"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 中国在各个方向前5%论文影响力占比</h4>
			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-effect-fangxiang" >

			</div>
			<script>
				var myChart_effect_fangxiang = echarts.init(document.getElementById('echarts-effect-fangxiang'));

				setTimeout(function () {

					option = {
						legend: {type: 'scroll',
										top:25,},
						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							},
							{
								show: true,
								yAxisIndex: 0,
								filterMode: 'empty',
								width: 30,
								height: '80%',
								showDataShadow: false,
								left: '93%'
							}
						],

						dataset: {
							source: [
							['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
								['ML',0.0,0.0,0.0,0.0,0.0,0.0,0.05555555555555555,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.06451612903225806,0.10714285714285714,0.2571428571428571,0.10344827586206896,0.08823529411764706,0.24324324324324326,0.07692307692307693,0.16326530612244897,0.05660377358490566,0.18181818181818182,0.06172839506172839,0.10606060606060606,0.1111111111111111,0.10126582278481013,0.25925925925925924
								],
								['CV',0.16666666666666666,0.0,0.0,0.07142857142857142,0.0,0.0,0.0,0.0,0.0625,0.10526315789473684,0.0,0.14285714285714285,0.0,0.1,0.0,0.0,0.0,0.16666666666666666,0.24,0.06060606060606061,0.043478260869565216,0.15,0.18181818181818182,0.28,0.2413793103448276,0.3230769230769231,0.25,0.14457831325301204,0.18571428571428572,0.32075471698113206
								],
								['NLP',0.0,0.0,0.06666666666666667,0.0,0.0625,0.0,0.0,0.0,0.0625,0.0,0.0625,0.0,0.058823529411764705,0.0,0.0,0.0,0.06666666666666667,0.1,0.043478260869565216,0.23529411764705882,0.16,0.09090909090909091,0.11538461538461539,0.14285714285714285,0.2727272727272727,0.425531914893617,0.31666666666666665,0.2,0.23595505617977527,0.8181818181818182
								],
								['ROBOT',0.0,0.0,0.0,0.04,0.0,0.07142857142857142,0.03125,0.030303030303030304,0.09375,0.06896551724137931,0.023255813953488372,0.045454545454545456,0.05454545454545454,0.1,0.018518518518518517,0.023255813953488372,0.027777777777777776,0.044444444444444446,0.027777777777777776,0.0,0.0,0.01818181818181818,0.0,0.02127659574468085,0.06557377049180328,0.03571428571428571,0.13953488372093023,0.09803921568627451,0.058823529411764705,0.0
								],
								['AGENT',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.058823529411764705,0.045454545454545456,0.0,0.0,0.0,0.06666666666666667,0.0,0.043478260869565216,0.07692307692307693,0.0
								],
								['KR',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
								]
							]

						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'比例'},
						// grid: {top: '55%'},
						series: [
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
							{type: 'line', smooth: true, seriesLayoutBy: 'row'},
						]
					};


					myChart_effect_fangxiang.setOption(option);

				});

			</script>
		</div>

	</div>
</section>









<div id="guoji" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 国际合作分析</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class=" py-5" id="jieshi5"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p>本节将发表论文的单位分为纯国内团队，国内团队主导，外国团队主导三类主体，以中国科研机构与国外机构合作的论文数量及占比作为合作强度的评价指标。
					同时，评价方法强调合作关系中的主导作用，分析中国科研机构与国外机构的合作方式演化趋势。</p>
				<p >在2008年之前，纯国内团队，国内主导，国外主导的论文发表数量相差不大，在2008年之后，纯国内团队的发表数量及占比都大幅提高，中国独立研发能力开始变强。
					同时2010年之后，中国主导合作发表和国外主导合作发表文章数量有整体上升趋势，说明我国科研单位国际合作变得愈加频繁。
				</p>
			</div>
		</div>
	</div>
</section>
<section class=" py-5" id="about-guoji-top10"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> top10合作国家</h4>
			</div>
			<div class="col-lg-6" style="height: 400px;padding-top: 2rem" id="echarts-top10" >

			</div>
			<script>
				var top10guojia = echarts.init(document.getElementById('echarts-top10'));
				option = {

					legend: {
						orient: 'vertical',
						right: 70,
						top:0,
						data: ['United States','Singapore','Australia','Canada','Japan','United Kingdom','USA','Germany','Italy','France','India'	]
					},
					series: [
						{
							name: '合作国家',
							type: 'pie',
							radius: ['50%', '70%'],
							avoidLabelOverlap: false,
							label: {
								show: true,
								position: 'outside',
								formatter: '{b}: {c} ({d}%)'
							},
							emphasis: {
								label: {
									show: true,
									fontSize: '20',
									fontWeight: 'bold',
									
								}
							},
							labelLine: {
								show: true
							},
							data: [
								{value: 6124, name: 'United States'},
								{value: 841, name: 'Singapore'},
								{value: 699, name: 'Australia'},
								{value: 539, name: 'Canada'},
								{value: 490, name: 'Japan'},
								{value: 487, name: 'United Kingdom'},
								{value: 484, name: 'Germany'},
								{value: 253, name: 'Italy'},
								{value: 225, name: 'France'},
								{value: 220, name: 'India'}
							]
						}
					]
				};



				top10guojia.setOption(option);

			</script>
		</div>

	</div>
</section>
<section class=" py-5" id="about-guoji"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 中国参与的论文发表</h4>
			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-guojihezuo" >

			</div>
			<script>
				var guojihezuo = echarts.init(document.getElementById('echarts-guojihezuo'));


				setTimeout(function () {

					option = {
						legend: {type: 'scroll',
										top:25,},
						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							}
						],
						dataset: {
							source: [
							['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
								['纯国内团队', 16.0,18.0,24.0,29.0,29.0,30.0,50.0,37.0,49.0,62.0,64.0,65.0,74.0,118.0,95.0,82.0,55.0,178.0,132.0,159.0,138.0,231.0,192.0,309.0,382.0,458.0,526.0,564.0,822.0,1102.0],
								['中国主导合作发表', 7.0,4.0,13.0,14.0,19.0,18.0,18.0,16.0,29.0,39.0,38.0,50.0,39.0,51.0,49.0,42.0,39.0,129.0,109.0,87.0,84.0,134.0,101.0,202.0,128.0,303.0,269.0,357.0,378.0,413.0],
								['国外主导合作发表',6.0,9.0,4.0,13.0,19.0,11.0,11.0,16.0,34.0,39.0,54.0,38.0,44.0,80.0,73.0,70.0,45.0,146.0,141.0,82.0,98.0,119.0,121.0,167.0,141.0,203.0,228.0,328.0,376.0,233.0]
							]
						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'数量（篇）'},

						grid: {width: '50%',left:'5%',top:100,},
							series: [
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{
									
									type: 'pie',
									id: 'pie',
									center:['78%','55%'],
									radius: ['20%', '35%'],
									avoidLabelOverlap: false,
									label: {
											show: true,
											position:'outside',
											formatter: '{b}:{@1990}({d}%)',

										},
									encode: {
										itemName: 'product',
										value: '1990',
										tooltip: '1990'
									},
									emphasis: {
										label: {
											show: true,
											fontWeight: 'bold',
											fontSize:15
										}
									},

								}
							]
					};

					guojihezuo.on('updateAxisPointer', function (event) {
						var xAxisInfo = event.axesInfo[0];
						if (xAxisInfo) {
							var dimension = xAxisInfo.value + 1;
							guojihezuo.setOption({
								series: {
									id: 'pie',
									label: {
										formatter: '{b}:{@1990}({d}%)'
									},
									encode: {
										value: dimension,
										tooltip: dimension
									}
								}
							});
						}
					});

					guojihezuo.setOption(option);

				});

			</script>
		</div>

	</div>
</section>
<section class=" py-5" id="about-guoji-fangxiang"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 各方向国际合作的论文发表</h4>
			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-guojihezuofangxiang" >

			</div>
			<script>
				var guojihezuofangxiang = echarts.init(document.getElementById('echarts-guojihezuofangxiang'));


				setTimeout(function () {

					option = {
						legend: {type: 'scroll',
										top:25,},
						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							},

						],
						dataset: {
							source: [
							['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
										'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
										'2013','2014','2015','2016','2017','2018','2019'],
								['ML', 7.0,3.0,4.0,3.0,5.0,8.0,7.0,5.0,5.0,17.0,14.0,17.0,12.0,24.0,26.0,22.0,31.0,43.0,45.0,74.0,69.0,71.0,99.0,86.0,73.0,90.0,105.0,189.0,165.0,120.0],
								['CV', 5.0,1.0,0.0,12.0,6.0,5.0,4.0,5.0,12.0,23.0,19.0,21.0,1.0,24.0,3.0,13.0,0.0,124.0,89.0,51.0,35.0,68.0,23.0,154.0,66.0,223.0,168.0,296.0,288.0,416.0],
								['NLP', 1.0,4.0,2.0,0.0,2.0,0.0,0.0,3.0,2.0,2.0,7.0,1.0,10.0,7.0,7.0,2.0,7.0,24.0,32.0,21.0,29.0,29.0,41.0,84.0,61.0,127.0,128.0,126.0,217.0,65.0],
								['ROBOT', 0.0,2.0,10.0,11.0,21.0,14.0,16.0,18.0,35.0,35.0,41.0,49.0,53.0,69.0,76.0,65.0,36.0,65.0,67.0,19.0,31.0,70.0,45.0,35.0,58.0,30.0,53.0,63.0,48.0,31.0],
								['AGENT',0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,3.0,0.0,4.0,0.0,3.0,2.0,1.0,7.0,1.0,13.0,0.0,4.0,8.0,9.0,4.0,9.0,8.0,22.0,35.0,10.0,21.0,12.0],
								['KR',0.0,3.0,1.0,0.0,4.0,2.0,2.0,1.0,6.0,1.0,7.0,0.0,4.0,5.0,9.0,3.0,9.0,6.0,17.0,0.0,10.0,6.0,10.0,1.0,3.0,14.0,8.0,1.0,15.0,2.0]
							]
						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'数量（篇）'},
						grid: {width: '50%',left:'5%',top:100,},
							series: [
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{
									
									type: 'pie',
									id: 'pie',
									center:['78%','55%'],
									radius: ['20%', '35%'],
									avoidLabelOverlap: false,
									label: {
											show: true,
											position:'outside',
											formatter: '{b}:{@1990}({d}%)',

										},
									encode: {
										itemName: 'product',
										value: '1990',
										tooltip: '1990'
									},
									emphasis: {
										label: {
											show: true,
											fontWeight: 'bold',
											fontSize:15
										}
									},

								}
							]
					};

					guojihezuofangxiang.on('updateAxisPointer', function (event) {
						var xAxisInfo = event.axesInfo[0];
						if (xAxisInfo) {
							var dimension = xAxisInfo.value + 1;
							guojihezuofangxiang.setOption({
								series: {
									id: 'pie',
									label: {
										formatter: '{b}: {@[' + dimension + ']} ({d}%)'
									},
									encode: {
										value: dimension,
										tooltip: dimension
									}
								}
							});
						}
					});

					guojihezuofangxiang.setOption(option);

				});

			</script>
		</div>

	</div>
</section>



<div id="xiaoqilunwen" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 校企发表论文情况分析</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class=" py-5" id="jieshi4	"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >高校/研究机构发表的论文数一直都处于主导地位，从2006年之后高校/研究机构主导，企业合作的论文数所占比例不断增高，
					从2007年的0.34%到19年的1.84%，增幅达540%。且企业发表的论文数占比也从2006年的0.048%增长到2019年的0.24%，增幅达500%。从2014年开始企业作者所占比例实现五连增，在2019年达到了9.29%，这是2000以后的最高值，说明企业开始注重培养研究型人才，提高独立研究能力。
					综上，高校/研究机构一直是论文发表的主力军，企业有较好的潜力。
				</p>
			</div>
		</div>
	</div>
</section>
<section class=" py-5" id="qiyehezuo"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 校企论文发表情况</h4>

			</div>
			<div class="col-lg-6" style="height: 50vh;width: 60vm;padding-top: 2rem" id="echarts-qiye" >

			</div>
			<script>
				var qiye= echarts.init(document.getElementById('echarts-qiye'));
				option = {

					tooltip: {
						trigger: 'axis',
						axisPointer: {
							type: 'shadow',
							label: {
								show: true
							}
						}
					},
					toolbox: {
						show: true,
						feature: {
							mark: {show: true},
							dataView: {show: true, readOnly: false},
							magicType: {show: true, type: ['line', 'bar']},
							restore: {show: true},
							saveAsImage: {show: true}
						}
					},
					calculable: true,
					dataZoom: [
						{
							show: true,
							start: 0,
							end: 100
						},
						{
							type: 'inside',
							start: 94,
							end: 100
						},
						{
							show: true,
							yAxisIndex: 0,
							filterMode: 'empty',
							width: 30,
							height: '80%',
							showDataShadow: false,
							left: '93%'
						}
					],
					legend: {
						data: ['企业发表', '高校/研究机构发表', '未知','高校/研究机构主导,企业合作','企业主导，高校/研究机构合作'],
						type: 'scroll',
						top:25
					},
					toolbox: {
						feature: {
							saveAsImage: {}
						}
					},
					grid: {
						left: '3%',
						right: '6%',
						bottom: '3%',
						containLabel: true
					},
					xAxis: [
						{
							name:'年份(年)',
							type: 'category',
							boundaryGap: false,
							data: 	['1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
								'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
								'2013','2014','2015','2016','2017','2018','2019'],
						}
					],
					yAxis: [
						{
							name:'占比',
							type: 'value'
						}
					],
					series: [
						{
							name: '企业发表',
							type: 'line',
							stack: '总量',
							areaStyle: {},
							data: [0.0,0.0008818342151675485,0.003246753246753247,0.0007189072609633358,0.002612671456564337,0.0,0.0012217470983506415,0.0014005602240896359,0.001142204454597373,0.001697792869269949,0.0015090543259557343,0.0011092623405435386,0.0017921146953405018,0.00046253469010175765,0.002109704641350211,0.001131648434552999,0.0004880429477794046,0.00281214848143982,0.0036188178528347406,0.0007616146230007616,0.001643655489809336,0.0021482277121374865,0.0017969451931716084,0.002332633543270352,0.003432494279176201,0.0013879250520471894,0.0025979216626698643,0.000962000962000962,0.0016925680873980612,0.0024424284717376133]
						},
						{
							name: '高校/研究机构发表',
							type: 'line',
							stack: '总量',
							areaStyle: {},
							data: [0.017676767676767676,0.01675485008818342,0.021915584415584416,0.021567217828900073,0.02612671456564337,0.023343373493975902,0.029932803909590716,0.025210084033613446,0.02741290691033695,0.04018109790605546,0.032696177062374245,0.040488075429839156,0.036290322580645164,0.0513413506012951,0.04177215189873418,0.03206337231233497,0.03074670571010249,0.05483689538807649,0.08242862887012464,0.08035034272658036,0.06081525312294543,0.08485499462943072,0.06858340820604972,0.08817354793561931,0.07231121281464531,0.07633587786259542,0.08772981614708233,0.07679974346641014,0.0873980612401908,0.09106769016050244]
						},
						{
							name: '未知',
							type: 'line',
							stack: '总量',
							areaStyle: {},
							data: [0.0,0.0008818342151675485,0.0016233766233766235,0.008626887131560028,0.0013063357282821686,0.009036144578313253,0.00794135613927917,0.00980392156862745,0.0074243289548829245,0.010186757215619695,0.011066398390342052,0.013311148086522463,0.008512544802867384,0.020351526364477335,0.01181434599156118,0.00867597133157299,0.008784773060029283,0.016591676040494937,0.003216726980297547,0.003427265803503427,0.0023011176857330702,0.003490870032223416,0.010182689427972447,0.015162118031257289,0.033409610983981694,0.0399028452463567,0.05515587529976019,0.055475388808722145,0.08001230958609017,0.1451500348918353]
						},
						{
							name: '高校/研究机构主导,企业合作',
							type: 'line',
							stack: '总量',
							areaStyle: {},
							data: [0.0008417508417508417,0.0,0.0016233766233766235,0.0007189072609633358,0.0,0.0007530120481927711,0.0006108735491753207,0.0,0.0057110222729868645,0.0005659309564233164,0.004024144869215292,0.003882418191902385,0.002240143369175627,0.005550416281221091,0.0016877637130801688,0.002640513013956997,0.003416300634455832,0.007592800899887514,0.0036188178528347406,0.0049504950495049506,0.003616042077580539,0.006176154672395274,0.005390835579514825,0.0074644273384651275,0.005949656750572082,0.008848022206800832,0.008393285371702638,0.00962000962000962,0.01354054469918449,0.01849267271458479]
						},
						{
							name: '企业主导，高校/研究机构合作',
							type: 'line',
							stack: '总量',
							areaStyle: {},
							data: [0.0008417508417508417,0.0008818342151675485,0.0,0.0,0.0,0.0007530120481927711,0.0012217470983506415,0.0014005602240896359,0.0017133066818960593,0.0028296547821165816,0.001006036217303823,0.0033277870216306157,0.0008960573476702509,0.0018501387604070306,0.002531645569620253,0.001131648434552999,0.002440214738897023,0.003655793025871766,0.003216726980297547,0.002284843869002285,0.0039447731755424065,0.0021482277121374865,0.0011979634621144056,0.003965477023559599,0.0009153318077803204,0.0026023594725884803,0.0037969624300559553,0.002565335898669232,0.003539006000923219,0.004012561060711794]
						},


					]
				};


				qiye.setOption(option)

			</script>
		</div>

	</div>
</section>
<section class=" py-5" id="about-zuozhe"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >

			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<h4> 中国高校，企业作者分析</h4>
			</div>
			<div class="col-lg-6" style="height: 500px;padding-top: 2rem" id="echarts-zuozhe" >

			</div>
			<script>
				var zuozhe = echarts.init(document.getElementById('echarts-zuozhe'));


				setTimeout(function () {

					option = {
						legend: {type: 'scroll',
							top:25,},
						tooltip: {
							trigger: 'axis',
							axisPointer: {
								type: 'shadow',
								label: {
									show: true
								}
							}
						},
						toolbox: {
							show: true,
							feature: {
								mark: {show: true},
								dataView: {show: true, readOnly: false},
								magicType: {show: true, type: ['line', 'bar']},
								restore: {show: true},
								saveAsImage: {show: true}
							}
						},
						calculable: true,
						dataZoom: [
							{
								show: true,
								start: 0,
								end: 100
							},
							{
								type: 'inside',
								start: 94,
								end: 100
							},

						],
						dataset: {
							source: [
								['product', '1990', '1991','1992', '1993','1994', '1995','1996', '1997','1998', '1999','2000',
									'2001', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012',
									'2013','2014','2015','2016','2017','2018','2019'],
								['企业作者', 0.022727272727272728,0.0425531914893617,0.16417910447761194,0.012195121951219513,0.056179775280898875,0.010638297872340425,0.040983606557377046,0.031496062992125984,0.08633093525179857,0.03398058252427184,0.0273972602739726,0.045112781954887216,0.048,0.033734939759036145,0.027247956403269755,0.03735632183908046,0.052,0.06550218340611354,0.05545927209705372,0.039383561643835614,0.06204379562043796,0.04442148760330578,0.047954866008462625,0.04726976365118175,0.025727826675693975,0.03654883163571001,0.047764849969381504,0.056368563685636856,0.06452938758734073,0.09293924466338259],
								['高校/科研机构作者', 0.6590909090909091,0.6595744680851063,0.6716417910447762,0.5121951219512195,0.7415730337078652,0.574468085106383,0.5655737704918032,0.5354330708661418,0.5755395683453237,0.5631067961165048,0.6118721461187214,0.47368421052631576,0.552,0.4963855421686747,0.5177111716621253,0.6580459770114943,0.68,0.6069868995633187,0.8526863084922011,0.8801369863013698,0.8503649635036497,0.9090909090909091,0.7729196050775741,0.7098614506927465,0.3913337846987136,0.6087477531455961,0.5903245560318432,0.7615176151761518,0.7538018906699548,0.7113300492610838],
								['未知', 0.3181818181818182,0.2978723404255319,0.16417910447761194,0.47560975609756095,0.20224719101123595,0.4148936170212766,0.39344262295081966,0.4330708661417323,0.3381294964028777,0.4029126213592233,0.3607305936073059,0.48120300751879697,0.4,0.46987951807228917,0.4550408719346049,0.3045977011494253,0.268,0.32751091703056767,0.09185441941074524,0.08047945205479452,0.08759124087591241,0.04648760330578512,0.17912552891396333,0.24286878565607173,0.5829383886255924,0.3547034152186938,0.3619105939987753,0.1821138211382114,0.1816687217427045,0.19573070607553367]
							]
						},
						xAxis: {type: 'category',name:'年份（年）'},
						yAxis: {gridIndex: 0,name:'比例'},
						grid: {width: '50%',left:'5%',top:100,},
							series: [
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{type: 'line', smooth: true, seriesLayoutBy: 'row'},
								{
									
									type: 'pie',
									id: 'pie',
									center:['78%','55%'],
									radius: ['20%', '35%'],
									avoidLabelOverlap: false,
									label: {
											show: true,
											position:'outside',
											formatter: '{b}:({d}%)',

										},
									encode: {
										itemName: 'product',
										value: '1990',
										tooltip: '1990'
									},
									emphasis: {
										label: {
											show: true,
											fontWeight: 'bold',
											fontSize:15
										}
									},

								}
							]
					};

					zuozhe.on('updateAxisPointer', function (event) {
						var xAxisInfo = event.axesInfo[0];
						if (xAxisInfo) {
							var dimension = xAxisInfo.value + 1;
							zuozhe.setOption({
								series: {
									id: 'pie',
									label: {
										formatter: '{b}:({d}%)'
									},
									encode: {
										value: dimension,
										tooltip: dimension
									}
								}
							});
						}
					});

					zuozhe.setOption(option);

				});

			</script>
		</div>

	</div>
</section>



<div id="about-guanjianci" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 研究热点分析</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				
				<p> 本节按五年一个时间段，对不同时间段的关键词进行了数量分析，同时使用Louvain社区发现算法(详见参考文献3）对每个时间段数量前15名的关键词进行技术生态链的划分。
					Louvain社区发现算法是一种基于图数据的社区发现算法，该算法的目标是最大化模块度。模块度也称模块化度量值，是目前常用的一种衡量网络社区结构强度的方法，
					Louvain算法开始时将每个原始节点都看成一个独立的社区，社区内的连边权重为0。之后分两个步骤进行迭代：<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp1.算法扫描数据中的所有节点，针对每个节点遍历该节点的所有邻居节点，衡量把该节点加入其邻居节点所在的社区所带来的模块度的收益。并选择对应最大收益的邻居节点，加入其所在的社区。这一过程化重复进行指导每一个节点的社区归属都不在发生变化。<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp2.对步骤1中形成的社区进行折叠，把每个社区折叠成一个单点，分别计算这些新生成的“社区点”之间的连边权重，以及社区内的所有点之间的连边权重之和。用于下一轮的步骤1。
				</p>
				<p>本小节的划分结果通过绘制关系图呈现，在所有关系图中，某关键词在当前时间段被使用次数越多，该节点越大；连接两个关键词的边的值为同时使用某两个关键词的论文数量。不同生态链的关键词使用不同颜色区分。使用弹性布局,某节点与其他节点联系程序越小，则节点偏离程度越大。有关分析结果如下：</p>
			</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >1990年至1994年，热点主要在机器人和计算机视觉方面，此时，和机器人有关的论文主要围绕运动控制学，路径规划，位置控制，平稳性，实时等技术进行讨论。和计算机视觉有关的论文主要围绕图像分割，对象识别，导航，鲁棒性，计算几何学等技术进行讨论。</p>

				<div class="col-lg-6" style="height: 500px;" id="echarts-diyijieduan" >

				</div>
				<script>
					var diyijieduan= echarts.init(document.getElementById('echarts-diyijieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 800, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
{name:'knowledge base',value:108,symbolSize:21,itemStyle:{color:'#FFFF00'},},
{name:'artificial intelligence',value:143,symbolSize:28,itemStyle:{color:'#551abb'},},
{name:'computer science',value:165,symbolSize:33,itemStyle:{color:'#551abb'},},
{name:'natural language',value:99,symbolSize:19,itemStyle:{color:'#FFFF00'},},
{name:'navigation',value:124,symbolSize:24,itemStyle:{color:'#551abb'},},
{name:'computer vision',value:452,symbolSize:90,itemStyle:{color:'#551abb'},},
{name:'shape',value:141,symbolSize:28,itemStyle:{color:'#551abb'},},
{name:'image segmentation',value:119,symbolSize:23,itemStyle:{color:'#551abb'},},
{name:'robustness',value:112,symbolSize:22,itemStyle:{color:'#551abb'},},
{name:'robot kinematics',value:171,symbolSize:34,itemStyle:{color:'#4F94CD'},},
{name:'computational geometry',value:114,symbolSize:22,itemStyle:{color:'#551abb'},},
{name:'object recognition',value:107,symbolSize:21,itemStyle:{color:'#551abb'},},
{name:'real time',value:113,symbolSize:22,itemStyle:{color:'#4F94CD'},},
{name:'motion control',value:139,symbolSize:27,itemStyle:{color:'#4F94CD'},},
{name:'motion planning',value:119,symbolSize:23,itemStyle:{color:'#4F94CD'},},
{name:'stability',value:137,symbolSize:27,itemStyle:{color:'#4F94CD'},},
{name:'position control',value:132,symbolSize:26,itemStyle:{color:'#4F94CD'},},
{name:'feedback',value:106,symbolSize:21,itemStyle:{color:'#4F94CD'},},
{name:'kinematics',value:210,symbolSize:42,itemStyle:{color:'#4F94CD'},},
{name:'robot control',value:165,symbolSize:33,itemStyle:{color:'#4F94CD'},},

				],
				links: [
		{source:'knowledge base',target:'artificial intelligence',value:'3',},
{source:'knowledge base',target:'computer science',value:'2',},
{source:'natural language',target:'knowledge base',value:'6',},
{source:'navigation',target:'computer vision',value:'27',},
{source:'natural language',target:'computer science',value:'1',},
{source:'computer vision',target:'artificial intelligence',value:'12',},
{source:'shape',target:'image segmentation',value:'13',},
{source:'robustness',target:'artificial intelligence',value:'4',},
{source:'robustness',target:'navigation',value:'6',},
{source:'robustness',target:'robot kinematics',value:'3',},
{source:'robustness',target:'computer vision',value:'19',},
{source:'navigation',target:'artificial intelligence',value:'9',},
{source:'robot kinematics',target:'artificial intelligence',value:'6',},
{source:'robot kinematics',target:'navigation',value:'9',},
{source:'robot kinematics',target:'computer vision',value:'12',},
{source:'computer vision',target:'computational geometry',value:'22',},
{source:'object recognition',target:'image segmentation',value:'13',},
{source:'object recognition',target:'computer science',value:'5',},
{source:'image segmentation',target:'computational geometry',value:'8',},
{source:'object recognition',target:'computational geometry',value:'9',},
{source:'real time',target:'computer vision',value:'16',},
{source:'robustness',target:'computer science',value:'7',},
{source:'shape',target:'object recognition',value:'14',},
{source:'shape',target:'robustness',value:'10',},
{source:'shape',target:'computational geometry',value:'12',},
{source:'robustness',target:'computational geometry',value:'4',},
{source:'object recognition',target:'computer vision',value:'34',},
{source:'image segmentation',target:'computer science',value:'4',},
{source:'navigation',target:'motion control',value:'3',},
{source:'motion control',target:'computer vision',value:'6',},
{source:'robot kinematics',target:'motion control',value:'14',},
{source:'computer vision',target:'computer science',value:'24',},
{source:'shape',target:'computer science',value:'16',},
{source:'image segmentation',target:'computer vision',value:'38',},
{source:'navigation',target:'computer science',value:'5',},
{source:'shape',target:'navigation',value:'3',},
{source:'object recognition',target:'artificial intelligence',value:'6',},
{source:'motion planning',target:'computer vision',value:'5',},
{source:'stability',target:'position control',value:'12',},
{source:'position control',target:'computer vision',value:'10',},
{source:'stability',target:'computer vision',value:'2',},
{source:'robustness',target:'image segmentation',value:'6',},
{source:'shape',target:'computer vision',value:'33',},
{source:'robustness',target:'motion control',value:'1',},
{source:'computer science',target:'computational geometry',value:'12',},
{source:'stability',target:'object recognition',value:'1',},
{source:'stability',target:'computational geometry',value:'2',},
{source:'feedback',target:'computer vision',value:'5',},
{source:'real time',target:'feedback',value:'3',},
{source:'real time',target:'computer science',value:'2',},
{source:'robot kinematics',target:'computer science',value:'10',},
{source:'real time',target:'artificial intelligence',value:'3',},
{source:'image segmentation',target:'artificial intelligence',value:'1',},
{source:'shape',target:'artificial intelligence',value:'3',},
{source:'stability',target:'image segmentation',value:'2',},
{source:'robot kinematics',target:'image segmentation',value:'1',},
{source:'kinematics',target:'computer vision',value:'5',},
{source:'position control',target:'motion control',value:'16',},
{source:'position control',target:'navigation',value:'8',},
{source:'shape',target:'robot control',value:'3',},
{source:'robot control',target:'computer vision',value:'5',},
{source:'real time',target:'computational geometry',value:'3',},
{source:'robustness',target:'object recognition',value:'2',},
{source:'robot control',target:'navigation',value:'10',},
{source:'shape',target:'real time',value:'1',},
{source:'computer science',target:'artificial intelligence',value:'5',},
{source:'kinematics',target:'artificial intelligence',value:'4',},
{source:'robustness',target:'real time',value:'2',},
{source:'natural language',target:'artificial intelligence',value:'1',},
{source:'motion control',target:'computer science',value:'3',},
{source:'motion planning',target:'kinematics',value:'14',},
{source:'position control',target:'feedback',value:'8',},
{source:'motion planning',target:'computational geometry',value:'11',},
{source:'position control',target:'kinematics',value:'8',},
{source:'motion planning',target:'computer science',value:'7',},
{source:'robot control',target:'kinematics',value:'15',},
{source:'kinematics',target:'feedback',value:'7',},
{source:'real time',target:'position control',value:'5',},
{source:'motion control',target:'feedback',value:'10',},
{source:'stability',target:'robustness',value:'8',},
{source:'stability',target:'kinematics',value:'13',},
{source:'robustness',target:'kinematics',value:'6',},
{source:'position control',target:'motion planning',value:'8',},
{source:'real time',target:'motion control',value:'5',},
{source:'robot kinematics',target:'computational geometry',value:'5',},
{source:'robot kinematics',target:'motion planning',value:'11',},
{source:'robot kinematics',target:'kinematics',value:'31',},
{source:'stability',target:'artificial intelligence',value:'5',},
{source:'real time',target:'navigation',value:'4',},
{source:'shape',target:'motion planning',value:'6',},
{source:'robot kinematics',target:'feedback',value:'6',},
{source:'stability',target:'robot kinematics',value:'9',},
{source:'stability',target:'feedback',value:'14',},
{source:'motion control',target:'kinematics',value:'17',},
{source:'robot control',target:'real time',value:'14',},
{source:'robot kinematics',target:'robot control',value:'14',},
{source:'robot kinematics',target:'real time',value:'5',},
{source:'stability',target:'motion control',value:'14',},
{source:'shape',target:'robot kinematics',value:'6',},
{source:'motion planning',target:'artificial intelligence',value:'3',},
{source:'stability',target:'motion planning',value:'4',},
{source:'motion planning',target:'motion control',value:'10',},
{source:'real time',target:'motion planning',value:'4',},
{source:'stability',target:'computer science',value:'6',},
{source:'stability',target:'robot control',value:'19',},
{source:'robot control',target:'feedback',value:'13',},
{source:'robustness',target:'robot control',value:'6',},
{source:'stability',target:'shape',value:'4',},
{source:'robot control',target:'computer science',value:'4',},
{source:'robot control',target:'motion planning',value:'5',},
{source:'kinematics',target:'computational geometry',value:'6',},
{source:'kinematics',target:'computer science',value:'7',},
{source:'shape',target:'feedback',value:'1',},
{source:'stability',target:'real time',value:'2',},
{source:'navigation',target:'image segmentation',value:'4',},
{source:'motion planning',target:'feedback',value:'5',},
{source:'robot control',target:'motion control',value:'11',},
{source:'navigation',target:'feedback',value:'3',},
{source:'navigation',target:'motion planning',value:'6',},
{source:'object recognition',target:'knowledge base',value:'2',},
{source:'feedback',target:'computer science',value:'3',},
{source:'robustness',target:'feedback',value:'3',},
{source:'robot control',target:'computational geometry',value:'4',},
{source:'navigation',target:'computational geometry',value:'3',},
{source:'robot control',target:'artificial intelligence',value:'2',},
{source:'robustness',target:'motion planning',value:'2',},
{source:'kinematics',target:'image segmentation',value:'1',},
{source:'object recognition',target:'kinematics',value:'1',},
{source:'robot control',target:'image segmentation',value:'1',},
{source:'robot control',target:'object recognition',value:'1',},
{source:'shape',target:'kinematics',value:'4',},
{source:'robot control',target:'position control',value:'6',},
{source:'robustness',target:'position control',value:'2',},
{source:'feedback',target:'artificial intelligence',value:'2',},
{source:'stability',target:'navigation',value:'4',},
{source:'motion control',target:'computational geometry',value:'1',},
{source:'robot kinematics',target:'position control',value:'1',},
{source:'shape',target:'position control',value:'2',},
{source:'position control',target:'computational geometry',value:'1',},
{source:'computational geometry',target:'artificial intelligence',value:'2',},
{source:'real time',target:'knowledge base',value:'1',},]
			}
		]
	};

					diyijieduan.setOption(option)
	
				</script>
</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >1995年到1999年，热点仍在机器人和计算机视觉，不同的是，在1990年到1994年和计算视觉有关的导航，实时技术开始向机器人领域偏移，机器人领域也有了机器工程学，遥控机器人等新技术的出现。</p>

				<div class="col-lg-6" style="height: 500px;" id="echarts-dierjieduan" >

				</div>
				<script>
					var dierjieduan= echarts.init(document.getElementById('echarts-dierjieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 1500, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
				{name:'real time',value:159,symbolSize:31,itemStyle:{color:'#FFFF00'},},
{name:'computer vision',value:346,symbolSize:69,itemStyle:{color:'#551abb'},},
{name:'robot control',value:185,symbolSize:37,itemStyle:{color:'#4F94CD'},},
{name:'stability',value:144,symbolSize:28,itemStyle:{color:'#4F94CD'},},
{name:'computational geometry',value:159,symbolSize:31,itemStyle:{color:'#551abb'},},
{name:'satisfiability',value:125,symbolSize:25,itemStyle:{color:'#4F94CD'},},
{name:'computer science',value:215,symbolSize:43,itemStyle:{color:'#551abb'},},
{name:'robustness',value:210,symbolSize:42,itemStyle:{color:'#551abb'},},
{name:'image segmentation',value:140,symbolSize:28,itemStyle:{color:'#551abb'},},
{name:'shape',value:199,symbolSize:39,itemStyle:{color:'#551abb'},},
{name:'object recognition',value:135,symbolSize:27,itemStyle:{color:'#551abb'},},
{name:'robot kinematics',value:268,symbolSize:53,itemStyle:{color:'#4F94CD'},},
{name:'motion estimation',value:167,symbolSize:33,itemStyle:{color:'#551abb'},},
{name:'kinematics',value:241,symbolSize:48,itemStyle:{color:'#4F94CD'},},
{name:'motion control',value:309,symbolSize:61,itemStyle:{color:'#4F94CD'},},
{name:'navigation',value:187,symbolSize:37,itemStyle:{color:'#FFFF00'},},
{name:'feedback',value:166,symbolSize:33,itemStyle:{color:'#4F94CD'},},
{name:'mechanical engineering',value:136,symbolSize:27,itemStyle:{color:'#4F94CD'},},
{name:'telerobotics',value:126,symbolSize:25,itemStyle:{color:'#4F94CD'},},
{name:'motion planning',value:139,symbolSize:27,itemStyle:{color:'#4F94CD'},},


				],
				links: [
				{source:'real time',target:'computer vision',value:'7',},
{source:'robot control',target:'real time',value:'7',},
{source:'stability',target:'computational geometry',value:'5',},
{source:'satisfiability',target:'computer science',value:'3',},
{source:'satisfiability',target:'computer vision',value:'5',},
{source:'computer vision',target:'computer science',value:'20',},
{source:'computer vision',target:'computational geometry',value:'17',},
{source:'robustness',target:'real time',value:'12',},
{source:'image segmentation',target:'computer vision',value:'23',},
{source:'shape',target:'object recognition',value:'13',},
{source:'object recognition',target:'computer vision',value:'24',},
{source:'robot kinematics',target:'computer vision',value:'9',},
{source:'robot kinematics',target:'object recognition',value:'3',},
{source:'shape',target:'image segmentation',value:'16',},
{source:'motion estimation',target:'computer vision',value:'24',},
{source:'shape',target:'computer science',value:'11',},
{source:'shape',target:'computational geometry',value:'10',},
{source:'object recognition',target:'image segmentation',value:'14',},
{source:'robustness',target:'computer vision',value:'22',},
{source:'robustness',target:'object recognition',value:'14',},
{source:'object recognition',target:'motion estimation',value:'6',},
{source:'robustness',target:'computer science',value:'14',},
{source:'robustness',target:'image segmentation',value:'11',},
{source:'kinematics',target:'computer vision',value:'3',},
{source:'robustness',target:'motion estimation',value:'12',},
{source:'robustness',target:'computational geometry',value:'10',},
{source:'shape',target:'computer vision',value:'28',},
{source:'motion estimation',target:'computer science',value:'10',},
{source:'shape',target:'motion estimation',value:'8',},
{source:'object recognition',target:'computational geometry',value:'4',},
{source:'real time',target:'image segmentation',value:'2',},
{source:'motion estimation',target:'image segmentation',value:'17',},
{source:'image segmentation',target:'computational geometry',value:'6',},
{source:'shape',target:'robustness',value:'22',},
{source:'motion estimation',target:'motion control',value:'6',},
{source:'motion control',target:'computer vision',value:'8',},
{source:'robustness',target:'navigation',value:'21',},
{source:'stability',target:'shape',value:'9',},
{source:'navigation',target:'motion control',value:'14',},
{source:'real time',target:'object recognition',value:'3',},
{source:'image segmentation',target:'computer science',value:'8',},
{source:'computer science',target:'computational geometry',value:'8',},
{source:'motion estimation',target:'feedback',value:'2',},
{source:'robot kinematics',target:'computer science',value:'8',},
{source:'navigation',target:'motion estimation',value:'7',},
{source:'satisfiability',target:'robustness',value:'1',},
{source:'robustness',target:'motion control',value:'12',},
{source:'motion control',target:'feedback',value:'22',},
{source:'real time',target:'motion estimation',value:'6',},
{source:'navigation',target:'computer vision',value:'8',},
{source:'stability',target:'computer vision',value:'4',},
{source:'motion estimation',target:'computational geometry',value:'5',},
{source:'satisfiability',target:'computational geometry',value:'2',},
{source:'object recognition',target:'navigation',value:'9',},
{source:'mechanical engineering',target:'computer vision',value:'2',},
{source:'robustness',target:'mechanical engineering',value:'2',},
{source:'shape',target:'real time',value:'4',},
{source:'motion control',target:'mechanical engineering',value:'16',},
{source:'stability',target:'robustness',value:'7',},
{source:'object recognition',target:'computer science',value:'3',},
{source:'robot control',target:'computational geometry',value:'5',},
{source:'robustness',target:'robot kinematics',value:'13',},
{source:'motion control',target:'computer science',value:'9',},
{source:'robot kinematics',target:'kinematics',value:'25',},
{source:'motion control',target:'kinematics',value:'30',},
{source:'mechanical engineering',target:'kinematics',value:'28',},
{source:'stability',target:'motion control',value:'20',},
{source:'shape',target:'feedback',value:'2',},
{source:'feedback',target:'computer science',value:'3',},
{source:'telerobotics',target:'stability',value:'12',},
{source:'stability',target:'feedback',value:'12',},
{source:'telerobotics',target:'feedback',value:'11',},
{source:'stability',target:'computer science',value:'1',},
{source:'telerobotics',target:'motion control',value:'16',},
{source:'robot kinematics',target:'motion control',value:'52',},
{source:'robot kinematics',target:'feedback',value:'23',},
{source:'motion control',target:'computational geometry',value:'9',},
{source:'mechanical engineering',target:'computational geometry',value:'9',},
{source:'motion planning',target:'computer science',value:'4',},
{source:'kinematics',target:'feedback',value:'13',},
{source:'robot kinematics',target:'real time',value:'14',},
{source:'robustness',target:'motion planning',value:'3',},
{source:'real time',target:'motion planning',value:'6',},
{source:'robot control',target:'motion control',value:'33',},
{source:'satisfiability',target:'mechanical engineering',value:'1',},
{source:'motion planning',target:'motion control',value:'31',},
{source:'robustness',target:'feedback',value:'6',},
{source:'stability',target:'robot control',value:'18',},
{source:'robot kinematics',target:'mechanical engineering',value:'15',},
{source:'mechanical engineering',target:'feedback',value:'7',},
{source:'robot kinematics',target:'motion planning',value:'23',},
{source:'satisfiability',target:'motion control',value:'4',},
{source:'real time',target:'navigation',value:'13',},
{source:'navigation',target:'image segmentation',value:'5',},
{source:'navigation',target:'kinematics',value:'5',},
{source:'stability',target:'robot kinematics',value:'15',},
{source:'telerobotics',target:'robot kinematics',value:'6',},
{source:'stability',target:'motion planning',value:'3',},
{source:'shape',target:'motion planning',value:'4',},
{source:'robustness',target:'kinematics',value:'8',},
{source:'motion planning',target:'computational geometry',value:'12',},
{source:'telerobotics',target:'kinematics',value:'3',},
{source:'robot control',target:'object recognition',value:'1',},
{source:'stability',target:'kinematics',value:'13',},
{source:'robot control',target:'navigation',value:'8',},
{source:'navigation',target:'motion planning',value:'17',},
{source:'motion planning',target:'kinematics',value:'7',},
{source:'kinematics',target:'computational geometry',value:'16',},
{source:'feedback',target:'computer vision',value:'3',},
{source:'shape',target:'kinematics',value:'8',},
{source:'robot control',target:'kinematics',value:'16',},
{source:'telerobotics',target:'computer vision',value:'5',},
{source:'real time',target:'motion control',value:'15',},
{source:'robot control',target:'mechanical engineering',value:'6',},
{source:'robot kinematics',target:'navigation',value:'19',},
{source:'robot kinematics',target:'robot control',value:'28',},
{source:'navigation',target:'feedback',value:'4',},
{source:'stability',target:'object recognition',value:'1',},
{source:'stability',target:'mechanical engineering',value:'10',},
{source:'object recognition',target:'mechanical engineering',value:'4',},
{source:'robot control',target:'computer science',value:'6',},
{source:'kinematics',target:'computer science',value:'9',},
{source:'kinematics',target:'image segmentation',value:'1',},
{source:'motion estimation',target:'kinematics',value:'2',},
{source:'robot kinematics',target:'motion estimation',value:'5',},
{source:'satisfiability',target:'motion planning',value:'3',},
{source:'motion planning',target:'feedback',value:'7',},
{source:'satisfiability',target:'kinematics',value:'5',},
{source:'satisfiability',target:'real time',value:'2',},
{source:'robot control',target:'feedback',value:'17',},
{source:'shape',target:'motion control',value:'13',},
{source:'robot kinematics',target:'computational geometry',value:'10',},
{source:'robustness',target:'robot control',value:'10',},
{source:'shape',target:'robot control',value:'4',},
{source:'robot control',target:'computer vision',value:'2',},
{source:'real time',target:'feedback',value:'4',},
{source:'telerobotics',target:'image segmentation',value:'1',},
{source:'telerobotics',target:'computer science',value:'1',},
{source:'telerobotics',target:'robot control',value:'9',},
{source:'motion estimation',target:'mechanical engineering',value:'2',},
{source:'real time',target:'kinematics',value:'3',},
{source:'object recognition',target:'motion control',value:'3',},
{source:'telerobotics',target:'real time',value:'4',},
{source:'motion planning',target:'motion estimation',value:'1',},
{source:'object recognition',target:'motion planning',value:'2',},
{source:'satisfiability',target:'robot kinematics',value:'7',},
{source:'shape',target:'navigation',value:'2',},
{source:'image segmentation',target:'feedback',value:'1',},
{source:'shape',target:'robot kinematics',value:'6',},
{source:'satisfiability',target:'navigation',value:'2',},
{source:'real time',target:'computer science',value:'2',},
{source:'mechanical engineering',target:'image segmentation',value:'1',},
{source:'telerobotics',target:'robustness',value:'1',},
{source:'motion planning',target:'mechanical engineering',value:'5',},
{source:'robot control',target:'motion estimation',value:'2',},
{source:'robot control',target:'motion planning',value:'3',},
{source:'satisfiability',target:'robot control',value:'1',},
{source:'shape',target:'mechanical engineering',value:'5',},
{source:'telerobotics',target:'motion planning',value:'2',},
{source:'navigation',target:'computer science',value:'5',},
{source:'telerobotics',target:'mechanical engineering',value:'2',},
{source:'telerobotics',target:'navigation',value:'2',},
{source:'navigation',target:'computational geometry',value:'1',},
{source:'satisfiability',target:'feedback',value:'2',},
{source:'stability',target:'navigation',value:'2',},
{source:'real time',target:'mechanical engineering',value:'1',},
{source:'navigation',target:'mechanical engineering',value:'1',},]
			}
		]
	};

	dierjieduan.setOption(option)
	
				</script>
</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >2000到2004年，在机器人方向，相关技术没有太大变化。和计算机视觉有关的论文开始围绕追踪等技术进行讨论。支持向量机，强化学习等机器学习技术已开始被广泛讨论。</p>
				<div class="col-lg-6" style="height: 500px;" id="echarts-disanjieduan" >

				</div>
				<script>
					var disanjieduan= echarts.init(document.getElementById('echarts-disanjieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 1000, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
				{name:'real time',value:209,symbolSize:41,itemStyle:{color:'#FFFF00'},},
{name:'machine learning',value:192,symbolSize:38,itemStyle:{color:'#FFFF00'},},
{name:'computer vision',value:294,symbolSize:58,itemStyle:{color:'#551abb'},},
{name:'reinforcement learning',value:203,symbolSize:40,itemStyle:{color:'#FFFF00'},},
{name:'computational complexity',value:147,symbolSize:29,itemStyle:{color:'#551abb'},},
{name:'satisfiability',value:155,symbolSize:31,itemStyle:{color:'#4F94CD'},},
{name:'robustness',value:130,symbolSize:26,itemStyle:{color:'#551abb'},},
{name:'computer science',value:241,symbolSize:48,itemStyle:{color:'#551abb'},},
{name:'feedback',value:178,symbolSize:35,itemStyle:{color:'#4F94CD'},},
{name:'robot kinematics',value:324,symbolSize:64,itemStyle:{color:'#4F94CD'},},
{name:'stability',value:181,symbolSize:36,itemStyle:{color:'#4F94CD'},},
{name:'robot control',value:152,symbolSize:30,itemStyle:{color:'#4F94CD'},},
{name:'kinematics',value:196,symbolSize:39,itemStyle:{color:'#4F94CD'},},
{name:'support vector machine',value:135,symbolSize:27,itemStyle:{color:'#FFFF00'},},
{name:'navigation',value:214,symbolSize:42,itemStyle:{color:'#FFFF00'},},
{name:'tracking',value:144,symbolSize:28,itemStyle:{color:'#551abb'},},
{name:'position control',value:183,symbolSize:36,itemStyle:{color:'#4F94CD'},},
{name:'telerobotics',value:207,symbolSize:41,itemStyle:{color:'#4F94CD'},},
{name:'motion planning',value:194,symbolSize:38,itemStyle:{color:'#4F94CD'},},
{name:'motion control',value:427,symbolSize:85,itemStyle:{color:'#4F94CD'},},


				],
				links: [
				{source:'real time',target:'machine learning',value:'5',},
{source:'machine learning',target:'computer vision',value:'1',},
{source:'reinforcement learning',target:'machine learning',value:'9',},
{source:'reinforcement learning',target:'real time',value:'7',},
{source:'computer vision',target:'computational complexity',value:'7',},
{source:'satisfiability',target:'computational complexity',value:'4',},
{source:'robustness',target:'computer science',value:'7',},
{source:'machine learning',target:'computer science',value:'3',},
{source:'computer science',target:'computational complexity',value:'8',},
{source:'satisfiability',target:'feedback',value:'5',},
{source:'satisfiability',target:'computer science',value:'5',},
{source:'satisfiability',target:'real time',value:'4',},
{source:'robot kinematics',target:'computer science',value:'12',},
{source:'reinforcement learning',target:'computer science',value:'2',},
{source:'stability',target:'reinforcement learning',value:'1',},
{source:'stability',target:'computer science',value:'1',},
{source:'robot control',target:'kinematics',value:'14',},
{source:'robot kinematics',target:'reinforcement learning',value:'2',},
{source:'robustness',target:'reinforcement learning',value:'1',},
{source:'support vector machine',target:'machine learning',value:'9',},
{source:'support vector machine',target:'satisfiability',value:'2',},
{source:'machine learning',target:'computational complexity',value:'1',},
{source:'robustness',target:'computational complexity',value:'2',},
{source:'computer vision',target:'computer science',value:'15',},
{source:'stability',target:'robustness',value:'11',},
{source:'real time',target:'navigation',value:'11',},
{source:'robustness',target:'real time',value:'4',},
{source:'robustness',target:'navigation',value:'7',},
{source:'navigation',target:'computer vision',value:'4',},
{source:'robustness',target:'computer vision',value:'9',},
{source:'tracking',target:'computer vision',value:'24',},
{source:'tracking',target:'computational complexity',value:'2',},
{source:'real time',target:'computer vision',value:'8',},
{source:'satisfiability',target:'computer vision',value:'1',},
{source:'real time',target:'computer science',value:'5',},
{source:'tracking',target:'satisfiability',value:'1',},
{source:'robot control',target:'reinforcement learning',value:'2',},
{source:'robot control',target:'real time',value:'9',},
{source:'satisfiability',target:'machine learning',value:'1',},
{source:'support vector machine',target:'reinforcement learning',value:'1',},
{source:'tracking',target:'robot control',value:'3',},
{source:'robot control',target:'position control',value:'6',},
{source:'tracking',target:'position control',value:'7',},
{source:'robot kinematics',target:'kinematics',value:'27',},
{source:'navigation',target:'computer science',value:'6',},
{source:'tracking',target:'navigation',value:'3',},
{source:'tracking',target:'computer science',value:'3',},
{source:'tracking',target:'telerobotics',value:'4',},
{source:'tracking',target:'feedback',value:'11',},
{source:'telerobotics',target:'computer vision',value:'5',},
{source:'feedback',target:'computer vision',value:'5',},
{source:'telerobotics',target:'feedback',value:'6',},
{source:'robot kinematics',target:'motion planning',value:'21',},
{source:'telerobotics',target:'position control',value:'10',},
{source:'satisfiability',target:'motion control',value:'8',},
{source:'position control',target:'motion control',value:'27',},
{source:'robot kinematics',target:'motion control',value:'45',},
{source:'stability',target:'navigation',value:'6',},
{source:'tracking',target:'stability',value:'4',},
{source:'stability',target:'position control',value:'5',},
{source:'support vector machine',target:'motion control',value:'1',},
{source:'robustness',target:'motion planning',value:'1',},
{source:'motion planning',target:'computational complexity',value:'5',},
{source:'tracking',target:'motion control',value:'14',},
{source:'stability',target:'feedback',value:'16',},
{source:'position control',target:'feedback',value:'9',},
{source:'motion control',target:'kinematics',value:'18',},
{source:'robot kinematics',target:'robot control',value:'26',},
{source:'stability',target:'computer vision',value:'3',},
{source:'robot control',target:'feedback',value:'6',},
{source:'real time',target:'motion control',value:'18',},
{source:'telerobotics',target:'robot control',value:'9',},
{source:'kinematics',target:'feedback',value:'10',},
{source:'motion control',target:'computer science',value:'6',},
{source:'robustness',target:'robot control',value:'3',},
{source:'robustness',target:'position control',value:'4',},
{source:'navigation',target:'machine learning',value:'2',},
{source:'robot kinematics',target:'navigation',value:'13',},
{source:'reinforcement learning',target:'navigation',value:'4',},
{source:'robot kinematics',target:'machine learning',value:'1',},
{source:'robot kinematics',target:'real time',value:'5',},
{source:'telerobotics',target:'motion control',value:'16',},
{source:'motion planning',target:'computer science',value:'7',},
{source:'position control',target:'navigation',value:'10',},
{source:'robot kinematics',target:'feedback',value:'11',},
{source:'robustness',target:'robot kinematics',value:'10',},
{source:'motion control',target:'feedback',value:'24',},
{source:'robot kinematics',target:'computational complexity',value:'3',},
{source:'kinematics',target:'computer science',value:'7',},
{source:'robot kinematics',target:'position control',value:'5',},
{source:'real time',target:'motion planning',value:'9',},
{source:'motion planning',target:'motion control',value:'31',},
{source:'robot control',target:'motion control',value:'21',},
{source:'telerobotics',target:'satisfiability',value:'3',},
{source:'navigation',target:'kinematics',value:'2',},
{source:'navigation',target:'feedback',value:'5',},
{source:'satisfiability',target:'motion planning',value:'2',},
{source:'position control',target:'computer vision',value:'3',},
{source:'real time',target:'computational complexity',value:'1',},
{source:'navigation',target:'computational complexity',value:'4',},
{source:'tracking',target:'robot kinematics',value:'5',},
{source:'stability',target:'motion control',value:'26',},
{source:'robustness',target:'feedback',value:'2',},
{source:'navigation',target:'motion control',value:'9',},
{source:'telerobotics',target:'robustness',value:'2',},
{source:'tracking',target:'motion planning',value:'3',},
{source:'telerobotics',target:'real time',value:'4',},
{source:'motion planning',target:'feedback',value:'9',},
{source:'tracking',target:'robustness',value:'6',},
{source:'motion planning',target:'kinematics',value:'10',},
{source:'stability',target:'motion planning',value:'10',},
{source:'stability',target:'kinematics',value:'11',},
{source:'robot control',target:'navigation',value:'6',},
{source:'robot kinematics',target:'computer vision',value:'5',},
{source:'satisfiability',target:'position control',value:'2',},
{source:'satisfiability',target:'robot kinematics',value:'4',},
{source:'stability',target:'robot kinematics',value:'17',},
{source:'telerobotics',target:'kinematics',value:'3',},
{source:'motion control',target:'computer vision',value:'9',},
{source:'stability',target:'robot control',value:'8',},
{source:'navigation',target:'motion planning',value:'9',},
{source:'real time',target:'feedback',value:'2',},
{source:'position control',target:'kinematics',value:'5',},
{source:'stability',target:'real time',value:'3',},
{source:'telerobotics',target:'robot kinematics',value:'5',},
{source:'motion control',target:'computational complexity',value:'2',},
{source:'robustness',target:'motion control',value:'2',},
{source:'tracking',target:'real time',value:'4',},
{source:'robustness',target:'kinematics',value:'3',},
{source:'robot control',target:'motion planning',value:'5',},
{source:'robot control',target:'computer vision',value:'1',},
{source:'position control',target:'motion planning',value:'5',},
{source:'robot control',target:'computer science',value:'9',},
{source:'tracking',target:'kinematics',value:'3',},
{source:'real time',target:'kinematics',value:'4',},
{source:'kinematics',target:'computer vision',value:'2',},
{source:'tracking',target:'reinforcement learning',value:'1',},
{source:'telerobotics',target:'navigation',value:'2',},
{source:'motion planning',target:'computer vision',value:'3',},
{source:'telerobotics',target:'stability',value:'6',},
{source:'position control',target:'computer science',value:'2',},
{source:'kinematics',target:'computational complexity',value:'1',},
{source:'satisfiability',target:'kinematics',value:'2',},
{source:'telerobotics',target:'support vector machine',value:'1',},
{source:'telerobotics',target:'motion planning',value:'1',},
{source:'telerobotics',target:'computational complexity',value:'1',},
{source:'support vector machine',target:'computer science',value:'1',},
{source:'stability',target:'satisfiability',value:'1',},
{source:'satisfiability',target:'reinforcement learning',value:'1',},
{source:'reinforcement learning',target:'feedback',value:'1',},
{source:'support vector machine',target:'computer vision',value:'1',},
{source:'real time',target:'position control',value:'1',},
{source:'position control',target:'computational complexity',value:'2',},]
			}
		]
	};

	disanjieduan.setOption(option)
	
				</script>
</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >2005年到2009年， 和计算机视觉相关的论文关于图像重构和图像分类技术的讨论增多。数据挖掘技术已经被广泛讨论，并和计算机视觉方向的论文有较为紧密的联系。</p>
				<div class="col-lg-6" style="height: 500px;" id="echarts-disijieduan" >

				</div>
				<script>
					var disijieduan= echarts.init(document.getElementById('echarts-disijieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 1000, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
				{name:'shape',value:259,symbolSize:51,itemStyle:{color:'#FFFF00'},},
{name:'kinematics',value:206,symbolSize:41,itemStyle:{color:'#551abb'},},
{name:'computer science',value:208,symbolSize:41,itemStyle:{color:'#FFFF00'},},
{name:'robustness',value:301,symbolSize:60,itemStyle:{color:'#FFFF00'},},
{name:'robot kinematics',value:347,symbolSize:69,itemStyle:{color:'#551abb'},},
{name:'motion control',value:329,symbolSize:65,itemStyle:{color:'#551abb'},},
{name:'real time',value:264,symbolSize:52,itemStyle:{color:'#551abb'},},
{name:'navigation',value:226,symbolSize:45,itemStyle:{color:'#551abb'},},
{name:'satisfiability',value:204,symbolSize:40,itemStyle:{color:'#551abb'},},
{name:'proposed method',value:231,symbolSize:46,itemStyle:{color:'#FFFF00'},},
{name:'computer vision',value:449,symbolSize:89,itemStyle:{color:'#FFFF00'},},
{name:'pixel',value:182,symbolSize:36,itemStyle:{color:'#FFFF00'},},
{name:'image segmentation',value:279,symbolSize:55,itemStyle:{color:'#FFFF00'},},
{name:'feature extraction',value:343,symbolSize:68,itemStyle:{color:'#FFFF00'},},
{name:'image reconstruction',value:198,symbolSize:39,itemStyle:{color:'#FFFF00'},},
{name:'reinforcement learning',value:187,symbolSize:37,itemStyle:{color:'#551abb'},},
{name:'machine learning',value:311,symbolSize:62,itemStyle:{color:'#FFFF00'},},
{name:'data mining',value:202,symbolSize:40,itemStyle:{color:'#FFFF00'},},
{name:'object recognition',value:181,symbolSize:36,itemStyle:{color:'#FFFF00'},},
{name:'image classification',value:180,symbolSize:36,itemStyle:{color:'#FFFF00'},},



				],
				links: [
				{source:'shape',target:'kinematics',value:'8',},
{source:'kinematics',target:'computer science',value:'3',},
{source:'robustness',target:'kinematics',value:'5',},
{source:'robot kinematics',target:'motion control',value:'43',},
{source:'real time',target:'navigation',value:'17',},
{source:'real time',target:'kinematics',value:'3',},
{source:'satisfiability',target:'robot kinematics',value:'6',},
{source:'motion control',target:'kinematics',value:'25',},
{source:'proposed method',target:'motion control',value:'5',},
{source:'navigation',target:'computer vision',value:'10',},
{source:'robot kinematics',target:'proposed method',value:'3',},
{source:'shape',target:'robot kinematics',value:'8',},
{source:'pixel',target:'image segmentation',value:'30',},
{source:'robustness',target:'feature extraction',value:'15',},
{source:'robustness',target:'pixel',value:'22',},
{source:'feature extraction',target:'computer science',value:'12',},
{source:'proposed method',target:'computer science',value:'4',},
{source:'proposed method',target:'feature extraction',value:'13',},
{source:'shape',target:'pixel',value:'15',},
{source:'shape',target:'robustness',value:'16',},
{source:'feature extraction',target:'computer vision',value:'43',},
{source:'shape',target:'image reconstruction',value:'13',},
{source:'pixel',target:'image reconstruction',value:'16',},
{source:'image reconstruction',target:'feature extraction',value:'8',},
{source:'robustness',target:'computer vision',value:'24',},
{source:'shape',target:'computer vision',value:'24',},
{source:'proposed method',target:'image reconstruction',value:'4',},
{source:'reinforcement learning',target:'machine learning',value:'6',},
{source:'proposed method',target:'machine learning',value:'4',},
{source:'real time',target:'machine learning',value:'5',},
{source:'real time',target:'data mining',value:'4',},
{source:'machine learning',target:'computer vision',value:'6',},
{source:'machine learning',target:'data mining',value:'9',},
{source:'reinforcement learning',target:'real time',value:'1',},
{source:'object recognition',target:'computer vision',value:'25',},
{source:'reinforcement learning',target:'proposed method',value:'1',},
{source:'satisfiability',target:'real time',value:'1',},
{source:'robustness',target:'robot kinematics',value:'15',},
{source:'image classification',target:'data mining',value:'5',},
{source:'pixel',target:'computer vision',value:'29',},
{source:'pixel',target:'object recognition',value:'7',},
{source:'robustness',target:'image segmentation',value:'11',},
{source:'navigation',target:'computer science',value:'12',},
{source:'real time',target:'computer science',value:'6',},
{source:'navigation',target:'image segmentation',value:'7',},
{source:'image segmentation',target:'data mining',value:'11',},
{source:'navigation',target:'data mining',value:'9',},
{source:'image classification',target:'feature extraction',value:'30',},
{source:'robustness',target:'data mining',value:'15',},
{source:'shape',target:'object recognition',value:'13',},
{source:'shape',target:'image segmentation',value:'20',},
{source:'object recognition',target:'image segmentation',value:'16',},
{source:'proposed method',target:'pixel',value:'4',},
{source:'image segmentation',target:'feature extraction',value:'20',},
{source:'object recognition',target:'feature extraction',value:'26',},
{source:'robot kinematics',target:'object recognition',value:'2',},
{source:'object recognition',target:'image reconstruction',value:'3',},
{source:'robot kinematics',target:'image reconstruction',value:'3',},
{source:'pixel',target:'machine learning',value:'6',},
{source:'object recognition',target:'machine learning',value:'4',},
{source:'machine learning',target:'image reconstruction',value:'2',},
{source:'image reconstruction',target:'computer vision',value:'20',},
{source:'pixel',target:'feature extraction',value:'10',},
{source:'image segmentation',target:'computer vision',value:'27',},
{source:'satisfiability',target:'computer vision',value:'2',},
{source:'robustness',target:'object recognition',value:'6',},
{source:'real time',target:'pixel',value:'4',},
{source:'shape',target:'feature extraction',value:'14',},
{source:'image classification',target:'computer vision',value:'13',},
{source:'proposed method',target:'data mining',value:'4',},
{source:'shape',target:'data mining',value:'14',},
{source:'shape',target:'proposed method',value:'2',},
{source:'data mining',target:'computer science',value:'9',},
{source:'image segmentation',target:'computer science',value:'3',},
{source:'shape',target:'computer science',value:'12',},
{source:'image segmentation',target:'image reconstruction',value:'9',},
{source:'machine learning',target:'image classification',value:'3',},
{source:'robustness',target:'image classification',value:'10',},
{source:'robustness',target:'image reconstruction',value:'16',},
{source:'shape',target:'image classification',value:'10',},
{source:'real time',target:'image classification',value:'4',},
{source:'robustness',target:'real time',value:'11',},
{source:'satisfiability',target:'robustness',value:'2',},
{source:'satisfiability',target:'image reconstruction',value:'2',},
{source:'feature extraction',target:'data mining',value:'20',},
{source:'image reconstruction',target:'data mining',value:'2',},
{source:'image classification',target:'computer science',value:'5',},
{source:'object recognition',target:'computer science',value:'6',},
{source:'satisfiability',target:'computer science',value:'2',},
{source:'machine learning',target:'feature extraction',value:'3',},
{source:'robustness',target:'machine learning',value:'3',},
{source:'proposed method',target:'computer vision',value:'10',},
{source:'pixel',target:'image classification',value:'5',},
{source:'pixel',target:'computer science',value:'2',},
{source:'image reconstruction',target:'computer science',value:'4',},
{source:'proposed method',target:'image classification',value:'3',},
{source:'real time',target:'motion control',value:'8',},
{source:'real time',target:'computer vision',value:'9',},
{source:'machine learning',target:'computer science',value:'5',},
{source:'computer vision',target:'computer science',value:'9',},
{source:'pixel',target:'data mining',value:'7',},
{source:'object recognition',target:'image classification',value:'17',},
{source:'image reconstruction',target:'image classification',value:'4',},
{source:'navigation',target:'image reconstruction',value:'5',},
{source:'machine learning',target:'image segmentation',value:'3',},
{source:'navigation',target:'feature extraction',value:'9',},
{source:'real time',target:'image segmentation',value:'4',},
{source:'data mining',target:'computer vision',value:'10',},
{source:'robustness',target:'computer science',value:'8',},
{source:'image segmentation',target:'image classification',value:'14',},
{source:'real time',target:'feature extraction',value:'15',},
{source:'kinematics',target:'image segmentation',value:'1',},
{source:'pixel',target:'navigation',value:'1',},
{source:'shape',target:'satisfiability',value:'3',},
{source:'robustness',target:'proposed method',value:'4',},
{source:'object recognition',target:'data mining',value:'3',},
{source:'satisfiability',target:'image segmentation',value:'1',},
{source:'proposed method',target:'image segmentation',value:'5',},
{source:'real time',target:'image reconstruction',value:'2',},
{source:'real time',target:'proposed method',value:'2',},
{source:'kinematics',target:'computer vision',value:'4',},
{source:'shape',target:'machine learning',value:'1',},
{source:'satisfiability',target:'reinforcement learning',value:'2',},
{source:'satisfiability',target:'feature extraction',value:'1',},
{source:'robustness',target:'navigation',value:'19',},
{source:'satisfiability',target:'kinematics',value:'7',},
{source:'navigation',target:'motion control',value:'22',},
{source:'reinforcement learning',target:'motion control',value:'4',},
{source:'robot kinematics',target:'reinforcement learning',value:'3',},
{source:'object recognition',target:'motion control',value:'1',},
{source:'robot kinematics',target:'kinematics',value:'32',},
{source:'motion control',target:'computer science',value:'4',},
{source:'shape',target:'real time',value:'4',},
{source:'reinforcement learning',target:'computer science',value:'2',},
{source:'kinematics',target:'data mining',value:'7',},
{source:'object recognition',target:'navigation',value:'3',},
{source:'robot kinematics',target:'real time',value:'10',},
{source:'robot kinematics',target:'computer science',value:'7',},
{source:'robot kinematics',target:'navigation',value:'13',},
{source:'robot kinematics',target:'data mining',value:'6',},
{source:'motion control',target:'computer vision',value:'2',},
{source:'navigation',target:'kinematics',value:'5',},
{source:'robustness',target:'motion control',value:'4',},
{source:'shape',target:'motion control',value:'9',},
{source:'shape',target:'navigation',value:'6',},
{source:'robot kinematics',target:'feature extraction',value:'2',},
{source:'satisfiability',target:'motion control',value:'5',},
{source:'proposed method',target:'navigation',value:'3',},
{source:'reinforcement learning',target:'navigation',value:'1',},
{source:'motion control',target:'image reconstruction',value:'2',},
{source:'robot kinematics',target:'computer vision',value:'3',},
{source:'motion control',target:'data mining',value:'3',},
{source:'robot kinematics',target:'pixel',value:'1',},
{source:'motion control',target:'image segmentation',value:'1',},
{source:'navigation',target:'image classification',value:'2',},
{source:'real time',target:'object recognition',value:'1',},
{source:'reinforcement learning',target:'feature extraction',value:'1',},
{source:'robot kinematics',target:'image segmentation',value:'2',},
{source:'motion control',target:'feature extraction',value:'1',},
{source:'machine learning',target:'kinematics',value:'2',},
{source:'robot kinematics',target:'machine learning',value:'1',},
{source:'reinforcement learning',target:'kinematics',value:'1',},
{source:'proposed method',target:'object recognition',value:'1',},
{source:'satisfiability',target:'data mining',value:'1',},
{source:'satisfiability',target:'machine learning',value:'1',},
{source:'reinforcement learning',target:'computer vision',value:'1',},]
			}
		]
	};

	disijieduan.setOption(option)
	
				</script>
	
</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >2010到2014年，和机器人有关的论文开始进行力控，轨迹，数学模型，优化等技术的热点讨论。和计算机视觉相关的论文开始了对姿态估计等估计技术的热点讨论。</p>

				<div class="col-lg-6" style="height: 500px;" id="echarts-diwujieduan" >

				</div>
				<script>
					var diwujieduan= echarts.init(document.getElementById('echarts-diwujieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 1000, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
				{name:'planning',value:232,symbolSize:46,itemStyle:{color:'#FFFF00'},},
{name:'optimization',value:286,symbolSize:57,itemStyle:{color:'#FFFF00'},},
{name:'machine learning',value:192,symbolSize:38,itemStyle:{color:'#551abb'},},
{name:'robustness',value:228,symbolSize:45,itemStyle:{color:'#551abb'},},
{name:'computer vision',value:426,symbolSize:85,itemStyle:{color:'#551abb'},},
{name:'image segmentation',value:366,symbolSize:73,itemStyle:{color:'#551abb'},},
{name:'graph theory',value:181,symbolSize:36,itemStyle:{color:'#551abb'},},
{name:'pose estimation',value:215,symbolSize:43,itemStyle:{color:'#551abb'},},
{name:'object recognition',value:222,symbolSize:44,itemStyle:{color:'#551abb'},},
{name:'estimation',value:246,symbolSize:49,itemStyle:{color:'#551abb'},},
{name:'shape',value:267,symbolSize:53,itemStyle:{color:'#551abb'},},
{name:'image classification',value:297,symbolSize:59,itemStyle:{color:'#551abb'},},
{name:'feature extraction',value:426,symbolSize:85,itemStyle:{color:'#551abb'},},
{name:'mathematical model',value:291,symbolSize:58,itemStyle:{color:'#FFFF00'},},
{name:'image reconstruction',value:210,symbolSize:42,itemStyle:{color:'#551abb'},},
{name:'proposed method',value:296,symbolSize:59,itemStyle:{color:'#551abb'},},
{name:'trajectory',value:365,symbolSize:73,itemStyle:{color:'#FFFF00'},},
{name:'force',value:341,symbolSize:68,itemStyle:{color:'#FFFF00'},},
{name:'motion control',value:319,symbolSize:63,itemStyle:{color:'#FFFF00'},},
{name:'robot kinematics',value:359,symbolSize:71,itemStyle:{color:'#FFFF00'},},



				],
				links: [
				{source:'planning',target:'optimization',value:'10',},
{source:'planning',target:'machine learning',value:'1',},
{source:'optimization',target:'machine learning',value:'3',},
{source:'robustness',target:'computer vision',value:'16',},
{source:'image segmentation',target:'graph theory',value:'22',},
{source:'pose estimation',target:'computer vision',value:'9',},
{source:'optimization',target:'graph theory',value:'10',},
{source:'object recognition',target:'computer vision',value:'20',},
{source:'image segmentation',target:'computer vision',value:'19',},
{source:'pose estimation',target:'estimation',value:'38',},
{source:'shape',target:'estimation',value:'8',},
{source:'image classification',target:'computer vision',value:'27',},
{source:'image classification',target:'feature extraction',value:'39',},
{source:'robustness',target:'feature extraction',value:'20',},
{source:'feature extraction',target:'computer vision',value:'34',},
{source:'machine learning',target:'feature extraction',value:'4',},
{source:'mathematical model',target:'image reconstruction',value:'4',},
{source:'machine learning',target:'image segmentation',value:'1',},
{source:'optimization',target:'object recognition',value:'4',},
{source:'robustness',target:'graph theory',value:'11',},
{source:'image segmentation',target:'estimation',value:'15',},
{source:'estimation',target:'computer vision',value:'14',},
{source:'machine learning',target:'computer vision',value:'7',},
{source:'robustness',target:'optimization',value:'11',},
{source:'mathematical model',target:'computer vision',value:'10',},
{source:'object recognition',target:'mathematical model',value:'3',},
{source:'shape',target:'computer vision',value:'13',},
{source:'pose estimation',target:'image segmentation',value:'9',},
{source:'shape',target:'proposed method',value:'7',},
{source:'robustness',target:'pose estimation',value:'7',},
{source:'mathematical model',target:'feature extraction',value:'9',},
{source:'optimization',target:'mathematical model',value:'8',},
{source:'optimization',target:'feature extraction',value:'12',},
{source:'proposed method',target:'image reconstruction',value:'5',},
{source:'robustness',target:'image reconstruction',value:'6',},
{source:'robustness',target:'proposed method',value:'3',},
{source:'robustness',target:'image segmentation',value:'18',},
{source:'image segmentation',target:'feature extraction',value:'32',},
{source:'object recognition',target:'image classification',value:'27',},
{source:'object recognition',target:'image segmentation',value:'21',},
{source:'object recognition',target:'feature extraction',value:'32',},
{source:'mathematical model',target:'estimation',value:'15',},
{source:'shape',target:'feature extraction',value:'19',},
{source:'shape',target:'image segmentation',value:'22',},
{source:'pose estimation',target:'feature extraction',value:'18',},
{source:'feature extraction',target:'estimation',value:'19',},
{source:'trajectory',target:'image reconstruction',value:'5',},
{source:'image classification',target:'estimation',value:'3',},
{source:'optimization',target:'estimation',value:'13',},
{source:'optimization',target:'image classification',value:'9',},
{source:'shape',target:'pose estimation',value:'8',},
{source:'proposed method',target:'feature extraction',value:'15',},
{source:'image reconstruction',target:'feature extraction',value:'18',},
{source:'mathematical model',target:'graph theory',value:'9',},
{source:'graph theory',target:'feature extraction',value:'7',},
{source:'graph theory',target:'computer vision',value:'12',},
{source:'robustness',target:'image classification',value:'6',},
{source:'pose estimation',target:'image classification',value:'8',},
{source:'robustness',target:'estimation',value:'17',},
{source:'proposed method',target:'image segmentation',value:'7',},
{source:'proposed method',target:'optimization',value:'9',},
{source:'mathematical model',target:'image classification',value:'6',},
{source:'shape',target:'graph theory',value:'5',},
{source:'image reconstruction',target:'graph theory',value:'6',},
{source:'shape',target:'image reconstruction',value:'15',},
{source:'proposed method',target:'pose estimation',value:'5',},
{source:'image segmentation',target:'image reconstruction',value:'10',},
{source:'trajectory',target:'computer vision',value:'5',},
{source:'optimization',target:'computer vision',value:'14',},
{source:'shape',target:'object recognition',value:'14',},
{source:'proposed method',target:'estimation',value:'10',},
{source:'machine learning',target:'graph theory',value:'3',},
{source:'image reconstruction',target:'image classification',value:'7',},
{source:'optimization',target:'image reconstruction',value:'7',},
{source:'trajectory',target:'optimization',value:'30',},
{source:'mathematical model',target:'image segmentation',value:'5',},
{source:'object recognition',target:'image reconstruction',value:'3',},
{source:'image reconstruction',target:'computer vision',value:'13',},
{source:'image segmentation',target:'image classification',value:'24',},
{source:'image reconstruction',target:'estimation',value:'8',},
{source:'proposed method',target:'image classification',value:'7',},
{source:'shape',target:'machine learning',value:'2',},
{source:'shape',target:'robustness',value:'14',},
{source:'robustness',target:'object recognition',value:'4',},
{source:'trajectory',target:'robustness',value:'13',},
{source:'proposed method',target:'computer vision',value:'11',},
{source:'proposed method',target:'mathematical model',value:'5',},
{source:'pose estimation',target:'image reconstruction',value:'6',},
{source:'trajectory',target:'object recognition',value:'2',},
{source:'force',target:'computer vision',value:'3',},
{source:'image segmentation',target:'force',value:'2',},
{source:'trajectory',target:'feature extraction',value:'13',},
{source:'object recognition',target:'graph theory',value:'3',},
{source:'pose estimation',target:'object recognition',value:'14',},
{source:'proposed method',target:'object recognition',value:'4',},
{source:'pose estimation',target:'graph theory',value:'6',},
{source:'machine learning',target:'image reconstruction',value:'3',},
{source:'machine learning',target:'image classification',value:'6',},
{source:'shape',target:'mathematical model',value:'12',},
{source:'pose estimation',target:'mathematical model',value:'7',},
{source:'shape',target:'optimization',value:'8',},
{source:'trajectory',target:'graph theory',value:'8',},
{source:'shape',target:'image classification',value:'6',},
{source:'image classification',target:'graph theory',value:'2',},
{source:'trajectory',target:'proposed method',value:'3',},
{source:'trajectory',target:'pose estimation',value:'3',},
{source:'optimization',target:'image segmentation',value:'6',},
{source:'proposed method',target:'graph theory',value:'6',},
{source:'robustness',target:'mathematical model',value:'7',},
{source:'pose estimation',target:'optimization',value:'4',},
{source:'trajectory',target:'mathematical model',value:'32',},
{source:'trajectory',target:'force',value:'20',},
{source:'trajectory',target:'motion control',value:'43',},
{source:'motion control',target:'force',value:'38',},
{source:'mathematical model',target:'force',value:'30',},
{source:'robot kinematics',target:'mathematical model',value:'13',},
{source:'trajectory',target:'robot kinematics',value:'46',},
{source:'robot kinematics',target:'motion control',value:'39',},
{source:'motion control',target:'machine learning',value:'1',},
{source:'robot kinematics',target:'planning',value:'21',},
{source:'trajectory',target:'planning',value:'42',},
{source:'shape',target:'force',value:'16',},
{source:'motion control',target:'image segmentation',value:'2',},
{source:'planning',target:'graph theory',value:'13',},
{source:'planning',target:'image segmentation',value:'2',},
{source:'robustness',target:'force',value:'7',},
{source:'mathematical model',target:'machine learning',value:'1',},
{source:'trajectory',target:'machine learning',value:'3',},
{source:'optimization',target:'motion control',value:'9',},
{source:'robot kinematics',target:'optimization',value:'17',},
{source:'shape',target:'motion control',value:'12',},
{source:'planning',target:'motion control',value:'13',},
{source:'shape',target:'planning',value:'4',},
{source:'pose estimation',target:'motion control',value:'1',},
{source:'robot kinematics',target:'feature extraction',value:'5',},
{source:'shape',target:'robot kinematics',value:'12',},
{source:'trajectory',target:'image segmentation',value:'2',},
{source:'planning',target:'force',value:'4',},
{source:'robot kinematics',target:'pose estimation',value:'4',},
{source:'robot kinematics',target:'estimation',value:'11',},
{source:'robot kinematics',target:'force',value:'13',},
{source:'robustness',target:'planning',value:'4',},
{source:'trajectory',target:'shape',value:'8',},
{source:'motion control',target:'mathematical model',value:'16',},
{source:'planning',target:'mathematical model',value:'9',},
{source:'motion control',target:'estimation',value:'3',},
{source:'optimization',target:'force',value:'9',},
{source:'image reconstruction',target:'force',value:'1',},
{source:'robustness',target:'machine learning',value:'1',},
{source:'proposed method',target:'motion control',value:'6',},
{source:'robot kinematics',target:'graph theory',value:'6',},
{source:'robustness',target:'motion control',value:'4',},
{source:'planning',target:'estimation',value:'1',},
{source:'robot kinematics',target:'machine learning',value:'1',},
{source:'force',target:'estimation',value:'4',},
{source:'proposed method',target:'planning',value:'1',},
{source:'robot kinematics',target:'object recognition',value:'2',},
{source:'object recognition',target:'estimation',value:'3',},
{source:'motion control',target:'computer vision',value:'1',},
{source:'robustness',target:'robot kinematics',value:'12',},
{source:'pose estimation',target:'machine learning',value:'1',},
{source:'robot kinematics',target:'image reconstruction',value:'1',},
{source:'machine learning',target:'force',value:'2',},
{source:'trajectory',target:'image classification',value:'2',},
{source:'motion control',target:'graph theory',value:'2',},
{source:'robot kinematics',target:'image classification',value:'1',},
{source:'planning',target:'feature extraction',value:'1',},
{source:'trajectory',target:'estimation',value:'2',},
{source:'motion control',target:'feature extraction',value:'1',},
{source:'planning',target:'image classification',value:'1',},
{source:'robot kinematics',target:'computer vision',value:'2',},
{source:'robot kinematics',target:'proposed method',value:'1',},
{source:'graph theory',target:'estimation',value:'1',},
{source:'proposed method',target:'machine learning',value:'1',},]
			}
		]
	};

	diwujieduan.setOption(option)
	
				</script>
</div>
		</div>
	</div>
</section>
<section class=" py-5" id="guanjianci"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >2015到2019年AI领域变化非常大，与深度神经网络，循环神经网络，卷积神经网络等神经网络技术的讨论成为热点话题，强化学习，深度学习等机器学习技术的讨论也变得更为广泛和紧密。在这个阶段有大量论文集中在算法研究，这些论文主要在大范围，大数，挑战性难题，新途径，训练集，计算复杂度，更好的表现等方面进行了讨论。</p>
				<div class="col-lg-6" style="height: 500px;" id="echarts-diliujieduan" >

				</div>
				<script>
					var diliujieduan= echarts.init(document.getElementById('echarts-diliujieduan'));

	
	
					option = {

tooltip: {
	show: true, // 默认显示
	showContent: true, // 是否显示提示框浮层
	trigger: 'item', // 触发类型，默认数据项触发
	triggerOn: 'mousemove', // 提示触发条件，mousemove鼠标移至触发，还有click点击触发
	alwaysShowContent: false, // 默认离开提示框区域隐藏，true为一直显示
	showDelay: 100, // 浮层显示的延迟，单位为 ms，默认没有延迟，也不建议设置。在 triggerOn 为 'mousemove' 时有效。
	hideDelay: 2000, // 浮层隐藏的延迟，单位为 ms，在 alwaysShowContent 为 true 的时候无效。
	enterable: false, // 鼠标是否可进入提示框浮层中，默认为false，如需详情内交互，如添加链接，按钮，可设置为 true。
	position: 'right', // 提示框浮层的位置，默认不设置时位置会跟随鼠标的位置。只在 trigger 为'item'的时候有效。
	confine: false, // 是否将 tooltip 框限制在图表的区域内。
	// 外层的 dom 被设置为 'overflow: hidden'，或者移动端窄屏，导致 tooltip 超出外界被截断时，此配置比较有用。
	transitionDuration: 0.2, // 提示框浮层的移动动画过渡时间，单位是秒，设置为 0 的时候会紧跟着鼠标移动。
},

		series: [
			{
	type: 'graph', // 关系图

	layout: 'force', // 图的布局，类型为力导图，'circular' 采用环形布局，见示例 Les Miserables
	legendHoverLink: true, // 是否启用图例 hover(悬停) 时的联动高亮。
	hoverAnimation: true, // 是否开启鼠标悬停节点的显示动画
	coordinateSystem: null, // 坐标系可选
	xAxisIndex: 0, // x轴坐标 有多种坐标系轴坐标选项
	yAxisIndex: 0, // y轴坐标 

	force: { // 力引导图基本配置
		// initLayout: , // 力引导的初始化布局，默认使用xy轴的标点
		repulsion: 1000, // 节点之间的斥力因子。支持数组表达斥力范围，值越大斥力越大。
		gravity: 0.02, // 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。
		edgeLength: 200, // 边的两个节点之间的距离，这个距离也会受 repulsion影响 。值越大则长度越长
		layoutAnimation: false // 因为力引导布局会在多次迭代后才会稳定，这个参数决定是否显示布局的迭代动画
		// 在浏览器端节点数据较多（>100）的时候不建议关闭，布局过程会造成浏览器假死。                        
	},
	roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启，true 为都开启。如果只想要开启缩放或者平移，可以设置成 'scale' 或者 'move'
	nodeScaleRatio: 0.6, // 鼠标漫游缩放时节点的相应缩放比例，当设为0时节点不随着鼠标的缩放而缩放
	draggable: true, // 节点是否可拖拽，只在使用力引导布局的时候有用。
	focusNodeAdjacency: true, // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。
				draggable: true,
				 edgeSymbol: ['none', 'none'], // 边两端的标记类型，可以是一个数组分别指定两端，也可以是单个统一指定。
	// 默认不显示标记，常见的可以设置为箭头，如下：edgeSymbol: ['circle', 'arrow']
	edgeSymbolSize: 10, // 边两端的标记大小，可以是一个数组分别指定两端，也可以是单个统一指定。
itemStyle: { // ========图形样式，有 normal 和 emphasis 两个状态。
		// normal 是图形在默认状态下的样式；
		// emphasis 是图形在高亮状态下的样式，比如在鼠标悬浮或者图例联动高亮时。
		normal: { // 默认样式
			label: {
				show: true
			},
			borderType: 'solid', // 图形描边类型，默认为实线，支持 'solid'（实线）, 'dashed'(虚线), 'dotted'（点线）。
			borderColor: 'rgba(205, 149, 12, 0.4)', // 设置图形边框为淡金色,透明度为0.4
			borderWidth: 2, // 图形的描边线宽。为 0 时无描边。
			opacity: 1 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5

		},
		emphasis: { // 高亮状态

		}
	},
	lineStyle: { // ========关系边的公用线条样式。
		normal: {
			color: 'black',
			width: '2', //线的粗细
			type: 'solid', // 线的类型 'solid'（实线）'dashed'（虚线）'dotted'（点线）
			curveness: 0.3, // 线条的曲线程度，从0到1
			opacity: 0.5 // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。默认0.5
		},
		emphasis: { // 高亮状态

		}
	},
	label: { // ========结点图形上的文本标签
		normal: {
			show: true, // 是否显示标签。
			position: 'inside', // 标签的位置。['50%', '50%'] [x,y]
			textStyle: { // 标签的字体样式
				color: 'red', // 字体颜色 #cde6c7 #d1c7b7 #d9d6c3 #d3d7d4
				fontStyle: 'normal', // 文字字体的风格 'normal'标准 'italic'斜体 'oblique' 倾斜
				fontWeight: 'bolder', // 'normal'标准，'bold'粗的，'bolder'更粗的，'lighter'更细的，或100 | 200 | 300 | 400...
				fontFamily: 'sans-serif', // 文字的字体系列
				fontSize: 12, // 字体大小
			}
		},
		emphasis: { // 高亮状态

		}
	},
	edgeLabel: { // ========连接线上的文本标签 
		normal: {
			show: false // 不显示连接线上的文字，如果显示只能显示结点的value值，而不是连接线的值
		},
		emphasis: { // 高亮状态

		}
	},
	
				data: [
				{name:'proposed method',value:893,symbolSize:100,itemStyle:{color:'#FFFF00'},},
{name:'artificial intelligence',value:155,symbolSize:31,itemStyle:{color:'#551abb'},},
{name:'computer vision',value:313,symbolSize:62,itemStyle:{color:'#4F94CD'},},
{name:'challenging problem',value:212,symbolSize:42,itemStyle:{color:'#FFFF00'},},
{name:'proposed algorithm',value:256,symbolSize:51,itemStyle:{color:'#FFFF00'},},
{name:'machine learning',value:235,symbolSize:47,itemStyle:{color:'#551abb'},},
{name:'new approach',value:160,symbolSize:32,itemStyle:{color:'#FFFF00'},},
{name:'wide range',value:221,symbolSize:44,itemStyle:{color:'#FFFF00'},},
{name:'superior performance',value:208,symbolSize:41,itemStyle:{color:'#FFFF00'},},
{name:'training data',value:337,symbolSize:67,itemStyle:{color:'#FFFF00'},},
{name:'deep learning',value:290,symbolSize:58,itemStyle:{color:'#4F94CD'},},
{name:'convolutional neural network',value:415,symbolSize:83,itemStyle:{color:'#4F94CD'},},
{name:'recurrent neural network',value:179,symbolSize:35,itemStyle:{color:'#4F94CD'},},
{name:'proposed model',value:308,symbolSize:61,itemStyle:{color:'#4F94CD'},},
{name:'large number',value:212,symbolSize:42,itemStyle:{color:'#FFFF00'},},
{name:'better performance',value:163,symbolSize:32,itemStyle:{color:'#458B74'},},
{name:'reinforcement learning',value:295,symbolSize:59,itemStyle:{color:'#458B74'},},
{name:'deep neural network',value:401,symbolSize:80,itemStyle:{color:'#4F94CD'},},
{name:'computational complexity',value:174,symbolSize:34,itemStyle:{color:'#FFFF00'},},
{name:'object detection',value:179,symbolSize:35,itemStyle:{color:'#4F94CD'},},



				],
				links: [
				{source:'proposed method',target:'artificial intelligence',value:'4',},
{source:'computer vision',target:'artificial intelligence',value:'5',},
{source:'computer vision',target:'challenging problem',value:'10',},
{source:'proposed algorithm',target:'challenging problem',value:'6',},
{source:'machine learning',target:'artificial intelligence',value:'11',},
{source:'proposed algorithm',target:'new approach',value:'2',},
{source:'wide range',target:'superior performance',value:'3',},
{source:'superior performance',target:'challenging problem',value:'7',},
{source:'wide range',target:'challenging problem',value:'8',},
{source:'superior performance',target:'proposed algorithm',value:'2',},
{source:'machine learning',target:'computer vision',value:'10',},
{source:'training data',target:'superior performance',value:'4',},
{source:'training data',target:'challenging problem',value:'10',},
{source:'wide range',target:'proposed algorithm',value:'2',},
{source:'proposed method',target:'proposed algorithm',value:'18',},
{source:'wide range',target:'proposed method',value:'12',},
{source:'wide range',target:'deep learning',value:'6',},
{source:'deep learning',target:'challenging problem',value:'5',},
{source:'proposed method',target:'challenging problem',value:'18',},
{source:'wide range',target:'convolutional neural network',value:'4',},
{source:'wide range',target:'artificial intelligence',value:'2',},
{source:'recurrent neural network',target:'proposed model',value:'8',},
{source:'convolutional neural network',target:'challenging problem',value:'7',},
{source:'proposed method',target:'new approach',value:'6',},
{source:'new approach',target:'large number',value:'5',},
{source:'proposed method',target:'better performance',value:'12',},
{source:'reinforcement learning',target:'proposed algorithm',value:'6',},
{source:'proposed method',target:'deep neural network',value:'25',},
{source:'superior performance',target:'proposed model',value:'7',},
{source:'superior performance',target:'proposed method',value:'20',},
{source:'proposed model',target:'challenging problem',value:'7',},
{source:'challenging problem',target:'artificial intelligence',value:'1',},
{source:'training data',target:'machine learning',value:'7',},
{source:'new approach',target:'convolutional neural network',value:'3',},
{source:'training data',target:'new approach',value:'3',},
{source:'training data',target:'convolutional neural network',value:'10',},
{source:'deep neural network',target:'convolutional neural network',value:'18',},
{source:'proposed model',target:'computer vision',value:'7',},
{source:'deep learning',target:'better performance',value:'2',},
{source:'recurrent neural network',target:'deep neural network',value:'6',},
{source:'proposed method',target:'machine learning',value:'5',},
{source:'wide range',target:'computer vision',value:'7',},
{source:'proposed model',target:'proposed method',value:'13',},
{source:'large number',target:'convolutional neural network',value:'4',},
{source:'superior performance',target:'artificial intelligence',value:'1',},
{source:'reinforcement learning',target:'computational complexity',value:'2',},
{source:'proposed method',target:'computer vision',value:'23',},
{source:'proposed algorithm',target:'large number',value:'4',},
{source:'training data',target:'proposed model',value:'6',},
{source:'large number',target:'challenging problem',value:'4',},
{source:'training data',target:'computer vision',value:'7',},
{source:'deep neural network',target:'deep learning',value:'21',},
{source:'training data',target:'deep learning',value:'5',},
{source:'training data',target:'deep neural network',value:'15',},
{source:'deep learning',target:'computer vision',value:'12',},
{source:'reinforcement learning',target:'recurrent neural network',value:'2',},
{source:'deep neural network',target:'computer vision',value:'18',},
{source:'proposed method',target:'convolutional neural network',value:'19',},
{source:'reinforcement learning',target:'proposed method',value:'9',},
{source:'wide range',target:'proposed model',value:'4',},
{source:'recurrent neural network',target:'convolutional neural network',value:'11',},
{source:'training data',target:'large number',value:'8',},
{source:'superior performance',target:'computer vision',value:'7',},
{source:'object detection',target:'deep learning',value:'6',},
{source:'deep neural network',target:'computational complexity',value:'2',},
{source:'recurrent neural network',target:'proposed method',value:'7',},
{source:'superior performance',target:'large number',value:'2',},
{source:'reinforcement learning',target:'large number',value:'4',},
{source:'training data',target:'proposed method',value:'17',},
{source:'training data',target:'artificial intelligence',value:'3',},
{source:'deep neural network',target:'artificial intelligence',value:'3',},
{source:'wide range',target:'recurrent neural network',value:'2',},
{source:'proposed model',target:'large number',value:'4',},
{source:'reinforcement learning',target:'better performance',value:'9',},
{source:'reinforcement learning',target:'computer vision',value:'2',},
{source:'wide range',target:'computational complexity',value:'8',},
{source:'training data',target:'better performance',value:'5',},
{source:'proposed model',target:'better performance',value:'4',},
{source:'convolutional neural network',target:'better performance',value:'4',},
{source:'proposed method',target:'deep learning',value:'9',},
{source:'object detection',target:'better performance',value:'2',},
{source:'challenging problem',target:'better performance',value:'3',},
{source:'proposed model',target:'deep neural network',value:'4',},
{source:'recurrent neural network',target:'better performance',value:'6',},
{source:'proposed model',target:'artificial intelligence',value:'2',},
{source:'recurrent neural network',target:'computer vision',value:'1',},
{source:'proposed model',target:'convolutional neural network',value:'10',},
{source:'superior performance',target:'reinforcement learning',value:'2',},
{source:'deep neural network',target:'better performance',value:'16',},
{source:'reinforcement learning',target:'deep neural network',value:'5',},
{source:'proposed method',target:'computational complexity',value:'5',},
{source:'machine learning',target:'deep neural network',value:'4',},
{source:'new approach',target:'computational complexity',value:'2',},
{source:'training data',target:'recurrent neural network',value:'3',},
{source:'wide range',target:'training data',value:'3',},
{source:'training data',target:'reinforcement learning',value:'4',},
{source:'proposed method',target:'large number',value:'13',},
{source:'computer vision',target:'computational complexity',value:'2',},
{source:'proposed model',target:'deep learning',value:'7',},
{source:'large number',target:'deep learning',value:'5',},
{source:'deep learning',target:'convolutional neural network',value:'14',},
{source:'object detection',target:'challenging problem',value:'2',},
{source:'proposed algorithm',target:'convolutional neural network',value:'6',},
{source:'new approach',target:'challenging problem',value:'3',},
{source:'proposed method',target:'object detection',value:'10',},
{source:'superior performance',target:'deep learning',value:'4',},
{source:'wide range',target:'object detection',value:'3',},
{source:'object detection',target:'convolutional neural network',value:'7',},
{source:'reinforcement learning',target:'deep learning',value:'4',},
{source:'training data',target:'object detection',value:'2',},
{source:'convolutional neural network',target:'computer vision',value:'15',},
{source:'superior performance',target:'better performance',value:'1',},
{source:'superior performance',target:'deep neural network',value:'8',},
{source:'object detection',target:'large number',value:'2',},
{source:'large number',target:'computer vision',value:'4',},
{source:'proposed algorithm',target:'machine learning',value:'4',},
{source:'proposed algorithm',target:'computer vision',value:'5',},
{source:'object detection',target:'computer vision',value:'6',},
{source:'new approach',target:'computer vision',value:'4',},
{source:'wide range',target:'deep neural network',value:'3',},
{source:'machine learning',target:'convolutional neural network',value:'1',},
{source:'deep neural network',target:'challenging problem',value:'3',},
{source:'proposed algorithm',target:'computational complexity',value:'6',},
{source:'convolutional neural network',target:'computational complexity',value:'1',},
{source:'computational complexity',target:'challenging problem',value:'1',},
{source:'training data',target:'computational complexity',value:'1',},
{source:'proposed algorithm',target:'better performance',value:'3',},
{source:'proposed model',target:'object detection',value:'1',},
{source:'proposed model',target:'computational complexity',value:'2',},
{source:'proposed algorithm',target:'deep neural network',value:'3',},
{source:'object detection',target:'deep neural network',value:'5',},
{source:'wide range',target:'new approach',value:'5',},
{source:'proposed algorithm',target:'object detection',value:'2',},
{source:'superior performance',target:'convolutional neural network',value:'7',},
{source:'proposed model',target:'proposed algorithm',value:'1',},
{source:'large number',target:'deep neural network',value:'2',},
{source:'wide range',target:'machine learning',value:'2',},
{source:'wide range',target:'large number',value:'6',},
{source:'reinforcement learning',target:'object detection',value:'2',},
{source:'computational complexity',target:'better performance',value:'2',},
{source:'wide range',target:'better performance',value:'2',},
{source:'recurrent neural network',target:'deep learning',value:'3',},
{source:'new approach',target:'better performance',value:'2',},
{source:'convolutional neural network',target:'artificial intelligence',value:'1',},
{source:'computer vision',target:'better performance',value:'2',},
{source:'superior performance',target:'object detection',value:'3',},
{source:'object detection',target:'computational complexity',value:'1',},
{source:'reinforcement learning',target:'convolutional neural network',value:'1',},
{source:'machine learning',target:'deep learning',value:'4',},
{source:'machine learning',target:'computational complexity',value:'2',},
{source:'superior performance',target:'recurrent neural network',value:'2',},
{source:'reinforcement learning',target:'machine learning',value:'1',},
{source:'reinforcement learning',target:'artificial intelligence',value:'2',},
{source:'wide range',target:'reinforcement learning',value:'3',},
{source:'reinforcement learning',target:'new approach',value:'1',},
{source:'better performance',target:'artificial intelligence',value:'1',},
{source:'machine learning',target:'large number',value:'1',},
{source:'superior performance',target:'new approach',value:'1',},
{source:'new approach',target:'machine learning',value:'3',},
{source:'large number',target:'artificial intelligence',value:'1',},
{source:'recurrent neural network',target:'challenging problem',value:'1',},
{source:'machine learning',target:'challenging problem',value:'1',},
{source:'recurrent neural network',target:'new approach',value:'1',},
{source:'computational complexity',target:'artificial intelligence',value:'1',},
{source:'new approach',target:'deep learning',value:'1',},
{source:'machine learning',target:'better performance',value:'1',},
{source:'training data',target:'proposed algorithm',value:'1',},
{source:'superior performance',target:'machine learning',value:'2',},
{source:'proposed algorithm',target:'deep learning',value:'1',},
{source:'reinforcement learning',target:'proposed model',value:'1',},]
			}
		]
	};

	diliujieduan.setOption(option)
	
				</script>
</div>
		</div>
	</div>
</section>

<div id="fanzhanbuju" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 研究方向发展布局分析</h4>
<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class="about-bottom"></section>
	<div class="container pb-lg-4">
		<div class="feature-grids row mt-lg-5 mt-4">
			<div class="col-lg-4  mt-4 p-sm-0">
				<div class="bottom-gd2-active p-lg-5 p-4">
					<span class="fa fa-star" aria-hidden="true"></span>
					<h3 class="my-3" style="font-size: 13px">优势研究方向:<br>>计算机视觉(CV)<br>>自然语言处理(NLP)</h3>
					<p>论文份额，影响力，增长率都处于较高水平</p>
				</div>
			</div>
			<div class="col-lg-4  mt-4 p-sm-0">
				<div class="bottom-gd p-lg-5 p-4">
					<span class="fa fa-chevron-up" aria-hidden="true"></span>
					<h3 class="my-3" style="font-size: 13px">弱势研究方向:<br>>知识表示(KR)<br><br></h3>
					<p>论文份额，影响力，增长率都处于较低水平</p>
				</div>
			</div>
			<div class="col-lg-4  mt-4 p-sm-0">
				<div class="bottom-gd2-active p-lg-5 p-4">
					<span class="fa fa-sign-out"></span>
					<h3 class="my-3" style="font-size: 13px">发展中研究方向:<br>>机器学习和神经网络(ML)<br><br></h3>
					<p>论文所占份额及影响力一般，增长率较高</p>
				</div>
			</div>
			<div class="col-lg-4  mt-4 p-sm-0">
				<div class="bottom-gd p-lg-5 p-4">
					<span class="fa fa-signal"></span>
					<h3 class="my-3" style="font-size: 13px">潜力研究方向:<br>>智能体(AGENT)<br>>机器人(ROBOT)</h3>
					<p>论文所占份额及影响力较低，增长率较高</p>
				</div>
			</div>
		</div>

	</div>
</section>
<!-- banner bottom grids -->
<section class=" py-5" id="jielun_1"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >通过分析中国各研究方向发展布局情况，可以为进一步调整发展方式提供依据，本文将人工智能领域分为计算机视觉，自然语言处理，机器学习和神经网络，智能体，机器人，知识表示，参考《中国科技论文统计结果:科研机构创新发展报告》（详见参考文献1）中的划分定义并作修改，根据波士顿矩阵方法，统计中国在各个方向的发表论文数量及被引用数情况，将六个方向从发文数量占有率，发文数量占有增长率，在影响力前5%论文中占有率，及在影响力前5%论文中占有增长率四个维度划分为优势研究方向，传统研究方向，潜力研究方向，弱势研究方向四个研究方向。其定义如下：<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp1.优势研究方向：2010年至2019年，该区研究方向发文数量和在影响力前5%论文中平均占有率达20%。<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp2.发展中研究方向：2010年至2019年，该区研究方向发文数量和在影响力前5%论文中平均占有率在10%~20%，平均增长率达20%。<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp3.弱势研究方向：2010年至2019年，该区研究方向发文数量和在影响力前5%论文中平均占有率不足10%，平均增长率不足20%。<br>
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp4.潜力研究方向：2010年至2019年，该区研究方向发文数量和在影响力前5%论文中平均占有率不足10%，平均增长率达20%。<br>
					2010-2019十年来各个方向论文数量和影响力情况如下表所示:
					</p>
				    <table style="width: 80%;" class="table">
						<thead>
							<tr>
								<th style="text-align: center;vertical-align: middle;">
									论文类别
								</th>
								<th style="text-align: center;vertical-align: middle;"> 
									论文数量平均占有率
								</th>
								<th style="text-align: center;vertical-align: middle;">
									发文数量平均占有增长率
								</th>
								<th style="text-align: center;vertical-align: middle;">
									在影响力前5%论文中平均占有率
								</th>
								<th style="text-align: center;vertical-align: middle;">
									在影响力前5%论文中平均占有增长率
								</th>
							</tr>
						</thead>

						<tr>
							<td>
								机器学习和神经网络
							</td>
							<td>
								12.13%
							</td>
							<td>
								13.03%
							</td>
							<td>
								17.02%
							</td>
							<td>
								51.06%
							</td>
				
						</tr>
						<tr>
							<td>
								计算机视觉
							</td>
							<td>
								20.86%
							</td>
							<td>
								17.31%
							</td>
							<td>
								26.51%
							</td>
							<td>
								53.81%
							</td>
				
						</tr>
						<tr>
							<td>
								自然语言处理
							</td>
							<td>
								21.09%
							</td>
							<td>
								18.62%
							</td>
							<td>
								34.73%
							</td>
							<td>
								50.97%
							</td>
				
						</tr>
						<tr>
							<td>
								机器人
							</td>
							<td>
								6.79%
							</td>
							<td>
								27.65%
							</td>
							<td>
								5.46%
							</td>
							<td>
								26.80%
							</td>
				
						</tr>
						<tr>
							<td>
								智能体
							</td>
							<td>
								4.50%
							</td>
							<td>
								31.51%
							</td>
							<td>
								3.64%
							</td>
							<td>
								-33.54%
							</td>
				
						</tr>
						<tr>
							<td>
								知识表示
							</td>
							<td>
								7.19%
							</td>
							<td>
								3.39%
							</td>
							<td>
								0.00%
							</td>
							<td>
								0.00%
							</td>
				
						</tr>
					</table>
				<p>按照之前的定义标准，优势研究方向包含计算机视觉和自然语言处理，发展中研究方向包含机器学习和神经网络，弱势研究方向为知识表示，潜力研究方向包含机器人和智能体。对于优势研究方向，可明确发展引导的路径，对于发展中研究方向，可完善管理机制以引导发展，对于潜力研究方向，可采用加大科研投入的方式进行引导，对于弱势研究方向，可考虑加强基础研究。</p>
				
			</div>
		</div>
	</div>
</section>



<div id="jielun" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 结论</h4>
	<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->


</div>
<section class=" py-5" id="jielun_1"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >近年来，世界整体人工智能领域都在快速发展，尤其在机器学习和神经网络，计算机视觉，自然语言处理领域，在各大国际会议的论文产出量大幅升高。伴随着世界AI发展大趋势，中国人工智能在2005年之后也进入快速增长时段，计算机视觉，自然语言处理方向的论文数量开始大幅增长，论文质量大幅提高，有越来越多新的单位发表计算机视觉，自然语言处理方面的论文，并逐渐发展为我国的优势研究方向。此外，中国在加强国际合作，增加国际合作论文产出的同时，独立研究能力有显著提高。</p>
			</div>
		</div>
	</div>
</section>
<div id="cankaowenxian" class="" style="margin:0 auto;text-align: center;margin-top: 30px;margin-bottom: 0px;width: 100%;background: #1da1f2;padding-top: 30px;padding-bottom: 30px">
	<h4 style="color: white"> 参考文献</h4>
<!--	<p style="color: white">每个会议录用论文数量、中国主导（第一作者是中国、第一单位是中国）的占比。</p>-->

</div>
<section class=" py-5" id="jielun_1"  >
	<div class="container py-md-5 " style="border: #e6e1e1 solid 5px;border-radius: 20px;" >
		<div class="line mt-lg-5 mt-4" >
			<div class="col-lg-6 cont-inf-w3ls" style="text-align: center">
				<p >
					[1]中国科技论文统计结果:科研机构创新发展报告[R].北京:中国科学技术信息研究所，2019.


				</p>
				<p >

					[2]中国计算机学会推荐国际学术会议和期刊目录-2019[S].中国计算机学会，2019.

				</p>
				<P>[3]Blondel.Fast unfolding of communities in large networks[J].Journal of Statistical Mechanics: Theory and Experiment,2008,30(2):155-168.</P>
			</div>
		</div>
	</div>
</section>

<section  id="test1">
	<div class="overlay-w3pvt-test py-5">
		<div class="container py-md-5">
			<div class="test-info text-center">
				<h3 class="my-md-2 my-3">Thanks</h3>

				<ul class="list-unstyled clients mb-3">
					<li>
						<a href="#">
							<span class="fa fa-star"></span>
						</a>
					</li>
					<li>
						<a href="#">
							<span class="fa fa-star"></span>
						</a>
					</li>
					<li>
						<a href="#">
							<span class="fa fa-star"></span>
						</a>
					</li>
					<li>
						<a href="#">
							<span class="fa fa-star-half-o"></span>
						</a>
					</li>
				</ul>
<!--				<p>"通过分析中国学者近年来在计算机国际会议的论文发表情况,挖掘活跃度大幅提升的重要时间节点,尝试从人才、产业、经费等方面的数据分析解释活跃起来的原因。-->
<!--					本文的分析对象为CCF推荐的A类和B类国际会议。内容分为三部分：中国学者参与情况、分析维度及解释、数据需求."</p>-->

			</div>
		</div>
	</div>
</section>
<footer id="contact" class="footer-content py-5">
	<div class="container-fluid py-lg-5 inner-sec-w3ls">
		<div class="row">
			<div class="col-lg-2 mt-lg-0 mt-5">
				<div class="footer-w3layouts">
					<h3 class="mb-3 w3layouts_title">Contact Us</h3>
					<hr>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">email:</a>
						</p>
					</div>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">1563377399@qq.com</a>
						</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>wechat</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>gcc1563377399</p>
					</div>

				</div>
			</div>
			<div class="col-lg-2 mt-lg-0 mt-5">
				<div class="footer-w3layouts">
					<h3 class="mb-3 w3layouts_title">Contact Us</h3>
					<hr>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">email:</a>
						</p>
					</div>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">1563377399@qq.com</a>
						</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>wechat</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>gcc1563377399</p>
					</div>

				</div>
			</div>
			<div class="col-lg-2 mt-lg-0 mt-5">
				<div class="footer-w3layouts">
					<h3 class="mb-3 w3layouts_title">Contact Us</h3>
					<hr>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">email:</a>
						</p>
					</div>
					<div class="last-w3pvt-contact">
						<p>
							<a href="mailto:example@email.com">1563377399@qq.com</a>
						</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>wechat</p>
					</div>
					<div class="last-w3pvt-contact my-2">
						<p>gcc1563377399</p>
					</div>

				</div>
			</div>
		</div>

	</div>
</footer>



</body>

</htML>
